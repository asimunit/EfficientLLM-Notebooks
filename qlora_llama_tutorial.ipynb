{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA: Quantized Low-Rank Adaptation with Llama 3.1\n",
    "\n",
    "This notebook provides a comprehensive guide to QLoRA (Quantized Low-Rank Adaptation), demonstrating how to fine-tune large language models like Llama 3.1 with extreme memory efficiency using 4-bit quantization.\n",
    "\n",
    "## Table of Contents\n",
    "1. QLoRA Theory and Quantization Fundamentals\n",
    "2. Setup and Dependencies\n",
    "3. Understanding 4-bit Quantization (NF4)\n",
    "4. Project: Fine-tuning Llama 3.1 for Customer Support Chatbot\n",
    "5. Data Preparation with Chat Templates\n",
    "6. QLoRA Configuration and Model Loading\n",
    "7. Memory Analysis and Optimization\n",
    "8. Training with QLoRA\n",
    "9. Inference and Model Merging\n",
    "10. Advanced QLoRA Techniques\n",
    "11. Performance Benchmarking\n",
    "12. Production Deployment Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. QLoRA Theory and Quantization Fundamentals\n",
    "\n",
    "### What is QLoRA?\n",
    "\n",
    "QLoRA (Quantized Low-Rank Adaptation) is an efficient fine-tuning method that combines:\n",
    "- **4-bit quantization** of the base model\n",
    "- **Low-rank adapters** for training\n",
    "- **Paged optimizers** for memory management\n",
    "\n",
    "### Key Innovations:\n",
    "\n",
    "**1. NF4 (Normal Float 4) Quantization:**\n",
    "```\n",
    "Traditional: FP16 (16 bits per parameter)\n",
    "QLoRA: NF4 (4 bits per parameter) = 4x memory reduction\n",
    "```\n",
    "\n",
    "**2. Double Quantization:**\n",
    "- Quantizes the quantization constants themselves\n",
    "- Additional 0.37 bits per parameter savings\n",
    "\n",
    "**3. Paged Optimizers:**\n",
    "- Uses NVIDIA unified memory\n",
    "- Handles optimizer states efficiently\n",
    "- Prevents out-of-memory errors during gradient spikes\n",
    "\n",
    "### Memory Comparison:\n",
    "- **Full Fine-tuning (FP16)**: 65B model = ~120GB\n",
    "- **LoRA (FP16)**: 65B model = ~80GB  \n",
    "- **QLoRA (NF4)**: 65B model = ~48GB (single GPU!)\n",
    "\n",
    "### Performance:\n",
    "QLoRA maintains 99.3% of full fine-tuning performance while using 4x less memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for QLoRA\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers>=4.31.0\n",
    "!pip install peft>=0.4.0\n",
    "!pip install datasets\n",
    "!pip install bitsandbytes>=0.39.0  # Critical for 4-bit quantization\n",
    "!pip install accelerate>=0.20.3\n",
    "!pip install trl  # For RLHF and chat training\n",
    "!pip install wandb  # For experiment tracking\n",
    "!pip install scipy  # For statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from datasets import Dataset, load_dataset\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional\n",
    "import warnings\n",
    "import gc\n",
    "import psutil\n",
    "import time\n",
    "from scipy import stats\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU capabilities\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"GPU {i} memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding 4-bit Quantization (NF4)\n",
    "\n",
    "### Normal Float 4 (NF4) Quantization\n",
    "\n",
    "NF4 is specifically designed for normally distributed weights in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate quantization concepts\n",
    "def demonstrate_quantization():\n",
    "    \"\"\"Visualize the effect of different quantization methods.\"\"\"\n",
    "    \n",
    "    # Generate normally distributed weights (typical in neural networks)\n",
    "    np.random.seed(42)\n",
    "    weights = np.random.normal(0, 1, 10000)\n",
    "    \n",
    "    print(\"Weight Distribution Analysis:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Mean: {weights.mean():.4f}\")\n",
    "    print(f\"Std: {weights.std():.4f}\")\n",
    "    print(f\"Min: {weights.min():.4f}\")\n",
    "    print(f\"Max: {weights.max():.4f}\")\n",
    "    \n",
    "    # Quantization comparison\n",
    "    def quantize_uniform(x, bits=4):\n",
    "        \"\"\"Uniform quantization.\"\"\"\n",
    "        levels = 2**bits - 1\n",
    "        x_min, x_max = x.min(), x.max()\n",
    "        scale = (x_max - x_min) / levels\n",
    "        quantized = np.round((x - x_min) / scale) * scale + x_min\n",
    "        return quantized\n",
    "    \n",
    "    # Compare quantization methods\n",
    "    uniform_4bit = quantize_uniform(weights, 4)\n",
    "    \n",
    "    # Calculate quantization errors\n",
    "    uniform_error = np.mean((weights - uniform_4bit)**2)\n",
    "    \n",
    "    print(f\"\\nQuantization Error Analysis:\")\n",
    "    print(f\"Uniform 4-bit MSE: {uniform_error:.6f}\")\n",
    "    print(f\"NF4 optimally distributes quantization levels for normal distributions\")\n",
    "    print(f\"NF4 reduces quantization error by ~20% compared to uniform quantization\")\n",
    "\n",
    "demonstrate_quantization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure 4-bit quantization with all optimizations\n",
    "def create_bnb_config():\n",
    "    \"\"\"Create optimized BitsAndBytes configuration for QLoRA.\"\"\"\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        # 4-bit quantization\n",
    "        load_in_4bit=True,\n",
    "        \n",
    "        # Use NF4 (Normal Float 4) quantization\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        \n",
    "        # Compute dtype for 4-bit base models\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        \n",
    "        # Double quantization for additional memory savings\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    return bnb_config\n",
    "\n",
    "# Create the quantization config\n",
    "quantization_config = create_bnb_config()\n",
    "\n",
    "print(\"QLoRA Quantization Configuration:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"4-bit loading: {quantization_config.load_in_4bit}\")\n",
    "print(f\"Quantization type: {quantization_config.bnb_4bit_quant_type}\")\n",
    "print(f\"Compute dtype: {quantization_config.bnb_4bit_compute_dtype}\")\n",
    "print(f\"Double quantization: {quantization_config.bnb_4bit_use_double_quant}\")\n",
    "print(f\"\\nExpected memory reduction: ~75% compared to FP16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Project: Fine-tuning Llama 3.1 for Customer Support Chatbot\n",
    "\n",
    "**Goal:** Create a customer support chatbot that can handle various inquiries with appropriate tone and helpful responses.\n",
    "\n",
    "**Key Features:**\n",
    "- Multi-turn conversation handling\n",
    "- Professional and empathetic responses\n",
    "- Domain-specific knowledge integration\n",
    "- Escalation detection and handling\n",
    "\n",
    "This project demonstrates QLoRA's capability for instruction following and conversation modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preparation with Chat Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer support training data\n",
    "customer_support_data = [\n",
    "    {\n",
    "        \"conversations\": [\n",
    "            {\"from\": \"human\", \"value\": \"Hi, I'm having trouble logging into my account. I keep getting an error message.\"},\n",
    "            {\"from\": \"assistant\", \"value\": \"I'm sorry to hear you're experiencing login issues. I'd be happy to help you resolve this. Can you please tell me what specific error message you're seeing? Also, have you tried resetting your password recently?\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"conversations\": [\n",
    "            {\"from\": \"human\", \"value\": \"I ordered a product 5 days ago but haven't received any shipping updates. What's going on?\"},\n",
    "            {\"from\": \"assistant\", \"value\": \"I understand your concern about the shipping delay, and I apologize for any inconvenience. Let me check the status of your order right away. Could you please provide me with your order number? In the meantime, I want to assure you that we're committed to getting your order to you as quickly as possible.\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"conversations\": [\n",
    "            {\"from\": \"human\", \"value\": \"Your website is terrible! It keeps crashing and I can't complete my purchase. This is ridiculous!\"},\n",
    "            {\"from\": \"assistant\", \"value\": \"I sincerely apologize for the frustrating experience you're having with our website. I understand how annoying technical issues can be, especially when you're trying to make a purchase. Let me help you complete your order right away. I can either walk you through an alternative method or process your order directly over this chat. Which option would work better for you?\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"conversations\": [\n",
    "            {\"from\": \"human\", \"value\": \"I need to return a product I bought last month. It doesn't fit properly.\"},\n",
    "            {\"from\": \"assistant\", \"value\": \"I'd be happy to help you with your return! We want to make sure you're completely satisfied with your purchase. Our return policy allows returns within 30 days of purchase for a full refund. Since you bought it last month, you should be well within that timeframe. Could you please provide your order number so I can start the return process for you?\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"conversations\": [\n",
    "            {\"from\": \"human\", \"value\": \"Can you explain how your subscription service works? I'm confused about the pricing.\"},\n",
    "            {\"from\": \"assistant\", \"value\": \"I'd be glad to explain our subscription service! We offer three tiers: Basic ($9.99/month), Premium ($19.99/month), and Enterprise ($39.99/month). Each tier includes different features and benefits. The Basic plan includes core features, Premium adds advanced analytics and priority support, while Enterprise includes everything plus dedicated account management. All plans can be cancelled anytime without fees. Would you like me to go into more detail about any specific tier?\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"conversations\": [\n",
    "            {\"from\": \"human\", \"value\": \"I think I was charged twice for the same order. Can you check this?\"},\n",
    "            {\"from\": \"assistant\", \"value\": \"I'm sorry to hear about the potential duplicate charge - that's definitely something we need to investigate immediately. Double charges can sometimes occur due to payment processing delays or technical glitches. I'll need to review your account and recent transactions to identify what happened. Could you please provide me with the order number and the amount you were charged? I'll also need the last four digits of the payment method used so I can locate the transactions quickly.\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"conversations\": [\n",
    "            {\"from\": \"human\", \"value\": \"Do you offer any discounts for students?\"},\n",
    "            {\"from\": \"assistant\", \"value\": \"Yes, we do offer a student discount! Students can receive 20% off our Premium subscription with valid student verification. To apply for the student discount, you'll need to verify your enrollment status through our education verification partner. The process is quick and secure - you'll just need to provide your school email address and upload a document confirming your enrollment. Once verified, the discount will be automatically applied to your account. Would you like me to guide you through the verification process?\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"conversations\": [\n",
    "            {\"from\": \"human\", \"value\": \"I'm having technical issues with your mobile app. It keeps freezing.\"},\n",
    "            {\"from\": \"assistant\", \"value\": \"I'm sorry you're experiencing freezing issues with our mobile app. Let's troubleshoot this together. First, could you tell me which device you're using and what version of the app you have installed? In the meantime, here are a few quick fixes you can try: 1) Force close the app and restart it, 2) Restart your device, 3) Check if there's an app update available in your app store. If these don't resolve the issue, we may need to clear the app cache or reinstall the app completely.\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Created {len(customer_support_data)} customer support conversation examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat template formatting for Llama 3.1\n",
    "def format_chat_template(conversations: List[Dict]) -> str:\n",
    "    \"\"\"Format conversations using Llama 3.1 chat template.\"\"\"\n",
    "    \n",
    "    formatted = \"<|begin_of_text|>\"\n",
    "    \n",
    "    # Add system message\n",
    "    formatted += \"<|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "    formatted += \"You are a helpful, professional, and empathetic customer support representative. \"\n",
    "    formatted += \"Always be polite, understanding, and solution-focused in your responses. \"\n",
    "    formatted += \"If you cannot resolve an issue, offer to escalate it to a specialist.\"\n",
    "    formatted += \"<|eot_id|>\"\n",
    "    \n",
    "    # Add conversation turns\n",
    "    for turn in conversations:\n",
    "        if turn[\"from\"] == \"human\":\n",
    "            formatted += f\"<|start_header_id|>user<|end_header_id|>\\n\\n{turn['value']}<|eot_id|>\"\n",
    "        elif turn[\"from\"] == \"assistant\":\n",
    "            formatted += f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{turn['value']}<|eot_id|>\"\n",
    "    \n",
    "    return formatted\n",
    "\n",
    "# Format the dataset\n",
    "def prepare_chat_dataset(examples: List[Dict]) -> Dataset:\n",
    "    \"\"\"Prepare the dataset with proper chat formatting.\"\"\"\n",
    "    \n",
    "    formatted_examples = []\n",
    "    \n",
    "    for example in examples:\n",
    "        formatted_text = format_chat_template(example[\"conversations\"])\n",
    "        formatted_examples.append({\"text\": formatted_text})\n",
    "    \n",
    "    return Dataset.from_list(formatted_examples)\n",
    "\n",
    "# Create the training dataset\n",
    "train_dataset = prepare_chat_dataset(customer_support_data)\n",
    "\n",
    "print(\"Sample formatted conversation:\")\n",
    "print(\"=\" * 50)\n",
    "print(train_dataset[0][\"text\"][:500] + \"...\")\n",
    "print(f\"\\nDataset created with {len(train_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. QLoRA Configuration and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "new_model = \"llama-3.1-8b-customer-support-qlora\"\n",
    "\n",
    "# Advanced LoRA configuration for QLoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=64,  # Higher rank for better performance\n",
    "    lora_alpha=16,  # Lower alpha relative to rank for stability\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "print(\"QLoRA Configuration:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"LoRA rank (r): {lora_config.r}\")\n",
    "print(f\"LoRA alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"LoRA dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"Target modules: {len(lora_config.target_modules)}\")\n",
    "print(f\"Scaling factor: {lora_config.lora_alpha / lora_config.r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory monitoring function\n",
    "def print_memory_usage(stage=\"\"):\n",
    "    \"\"\"Print current memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        gpu_allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        gpu_reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "        \n",
    "        print(f\"\\n{stage} Memory Usage:\")\n",
    "        print(f\"GPU Total: {gpu_memory:.1f} GB\")\n",
    "        print(f\"GPU Allocated: {gpu_allocated:.1f} GB ({gpu_allocated/gpu_memory*100:.1f}%)\")\n",
    "        print(f\"GPU Reserved: {gpu_reserved:.1f} GB ({gpu_reserved/gpu_memory*100:.1f}%)\")\n",
    "    \n",
    "    ram_usage = psutil.virtual_memory()\n",
    "    print(f\"RAM Usage: {ram_usage.used/1024**3:.1f} GB / {ram_usage.total/1024**3:.1f} GB ({ram_usage.percent:.1f}%)\")\n",
    "\n",
    "print_memory_usage(\"Initial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print_memory_usage(\"After tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with 4-bit quantization\n",
    "print(\"Loading model with QLoRA (4-bit quantization)...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print_memory_usage(\"After model loading\")\n",
    "print(\"\\nâœ… Model loaded successfully with 4-bit quantization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Add LoRA adapters\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print_memory_usage(\"After LoRA application\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Memory Analysis and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed memory analysis\n",
    "def analyze_model_memory(model):\n",
    "    \"\"\"Analyze memory usage of different model components.\"\"\"\n",
    "    \n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    quantized_params = 0\n",
    "    \n",
    "    print(\"Model Memory Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        total_params += param.numel()\n",
    "        \n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "            param_memory = param.numel() * param.element_size() / 1024**2\n",
    "            print(f\"Trainable: {name:40} | {param.numel():>12,} params | {param_memory:>8.1f} MB\")\n",
    "    \n",
    "    # Estimate memory savings from quantization\n",
    "    base_model_params = total_params - trainable_params\n",
    "    \n",
    "    # Memory calculations (approximate)\n",
    "    fp16_memory = total_params * 2 / 1024**3  # 2 bytes per parameter\n",
    "    qlora_memory = (base_model_params * 0.5 + trainable_params * 2) / 1024**3  # 4-bit base + 16-bit LoRA\n",
    "    \n",
    "    print(f\"\\nMemory Summary:\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n",
    "    print(f\"\\nMemory Estimates:\")\n",
    "    print(f\"Full FP16 model: {fp16_memory:.1f} GB\")\n",
    "    print(f\"QLoRA (4-bit + LoRA): {qlora_memory:.1f} GB\")\n",
    "    print(f\"Memory reduction: {(1 - qlora_memory/fp16_memory)*100:.1f}%\")\n",
    "\n",
    "analyze_model_memory(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training with QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments optimized for QLoRA\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=f\"./results_{new_model}\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,  # Small batch size for memory efficiency\n",
    "    gradient_accumulation_steps=8,  # Simulate larger batch size\n",
    "    optim=\"paged_adamw_32bit\",  # Paged optimizer for memory efficiency\n",
    "    save_steps=100,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,  # Higher learning rate for LoRA\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,  # Use bf16 instead\n",
    "    bf16=True,  # Better numerical stability with 4-bit\n",
    "    max_grad_norm=0.3,  # Gradient clipping\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,  # Group sequences by length for efficiency\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"none\",  # Disable wandb for this example\n",
    "    dataloader_num_workers=0,  # Avoid multiprocessing issues\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Epochs: {training_arguments.num_train_epochs}\")\n",
    "print(f\"Batch size: {training_arguments.per_device_train_batch_size}\")\n",
    "print(f\"Gradient accumulation: {training_arguments.gradient_accumulation_steps}\")\n",
    "print(f\"Effective batch size: {training_arguments.per_device_train_batch_size * training_arguments.gradient_accumulation_steps}\")\n",
    "print(f\"Learning rate: {training_arguments.learning_rate}\")\n",
    "print(f\"Optimizer: {training_arguments.optim}\")\n",
    "print(f\"Precision: {'BF16' if training_arguments.bf16 else 'FP16' if training_arguments.fp16 else 'FP32'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SFT trainer for chat fine-tuning\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,  # Don't pack sequences for chat format\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer initialized successfully!\")\n",
    "print_memory_usage(\"Before training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with progress monitoring\n",
    "print(\"Starting QLoRA training...\")\n",
    "print(\"Monitor GPU memory usage during training:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Clear cache before training\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâœ… Training completed in {training_time/60:.1f} minutes!\")\n",
    "print_memory_usage(\"After training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained adapter\n",
    "trainer.model.save_pretrained(new_model)\n",
    "tokenizer.save_pretrained(new_model)\n",
    "\n",
    "print(f\"âœ… Model saved to ./{new_model}\")\n",
    "print(\"\\nAdapter files saved:\")\n",
    "import os\n",
    "for file in os.listdir(new_model):\n",
    "    size = os.path.getsize(os.path.join(new_model, file)) / 1024**2\n",
    "    print(f\"  {file}: {size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference and Model Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference function\n",
    "def generate_customer_support_response(query: str, max_length: int = 512) -> str:\n",
    "    \"\"\"Generate customer support response using the fine-tuned model.\"\"\"\n",
    "    \n",
    "    # Format the input using chat template\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful, professional, and empathetic customer support representative. Always be polite, understanding, and solution-focused in your responses.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": query\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode and extract response\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = full_response.split(\"assistant\\n\\n\")[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"âœ… Inference function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fine-tuned model with various customer scenarios\n",
    "test_queries = [\n",
    "    \"I received a damaged product and need to return it urgently.\",\n",
    "    \"Can you help me understand why my premium features aren't working?\",\n",
    "    \"I'm very frustrated with your service and considering canceling my subscription.\",\n",
    "    \"How do I upgrade my account to access more storage?\",\n",
    "    \"I forgot my password and the reset email isn't coming through.\"\n",
    "]\n",
    "\n",
    "print(\"Testing the fine-tuned QLoRA model:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\nðŸ”¹ Test {i}:\")\n",
    "    print(f\"Customer: {query}\")\n",
    "    print(\"Support Agent:\", end=\" \")\n",
    "    \n",
    "    response = generate_customer_support_response(query)\n",
    "    print(response)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced QLoRA Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced LoRA techniques\n",
    "def create_advanced_lora_config():\n",
    "    \"\"\"Create advanced LoRA configuration with multiple techniques.\"\"\"\n",
    "    \n",
    "    # Different LoRA configurations for experimentation\n",
    "    configs = {\n",
    "        \"balanced\": LoraConfig(\n",
    "            r=32, lora_alpha=32, lora_dropout=0.1,\n",
    "            target_modules=[\"q_proj\", \"v_proj\"],\n",
    "            task_type=TaskType.CAUSAL_LM\n",
    "        ),\n",
    "        \n",
    "        \"comprehensive\": LoraConfig(\n",
    "            r=64, lora_alpha=16, lora_dropout=0.05,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            task_type=TaskType.CAUSAL_LM\n",
    "        ),\n",
    "        \n",
    "        \"efficient\": LoraConfig(\n",
    "            r=16, lora_alpha=32, lora_dropout=0.1,\n",
    "            target_modules=[\"q_proj\", \"v_proj\", \"o_proj\"],\n",
    "            task_type=TaskType.CAUSAL_LM\n",
    "        ),\n",
    "        \n",
    "        \"high_rank\": LoraConfig(\n",
    "            r=128, lora_alpha=64, lora_dropout=0.05,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "            task_type=TaskType.CAUSAL_LM\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return configs\n",
    "\n",
    "# Analyze different configurations\n",
    "lora_configs = create_advanced_lora_config()\n",
    "\n",
    "print(\"LoRA Configuration Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, config in lora_configs.items():\n",
    "    estimated_params = len(config.target_modules) * config.r * 2 * 4096  # Rough estimate\n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    print(f\"  Rank: {config.r}\")\n",
    "    print(f\"  Alpha: {config.lora_alpha}\")\n",
    "    print(f\"  Target modules: {len(config.target_modules)}\")\n",
    "    print(f\"  Estimated trainable params: ~{estimated_params:,}\")\n",
    "    print(f\"  Memory vs full fine-tuning: ~{estimated_params/8000000000*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model merging for deployment\n",
    "def merge_and_save_model(base_model_id: str, adapter_path: str, output_path: str):\n",
    "    \"\"\"Merge LoRA adapter with base model for deployment.\"\"\"\n",
    "    \n",
    "    print(f\"Merging adapter from {adapter_path} with base model {base_model_id}...\")\n",
    "    \n",
    "    # Load base model in fp16 for merging (not quantized)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Load and merge adapter\n",
    "    model_with_adapter = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    merged_model = model_with_adapter.merge_and_unload()\n",
    "    \n",
    "    # Save merged model\n",
    "    merged_model.save_pretrained(output_path)\n",
    "    \n",
    "    print(f\"âœ… Merged model saved to {output_path}\")\n",
    "    return merged_model\n",
    "\n",
    "# Example of how to merge (commented out to save memory)\n",
    "\"\"\"\n",
    "merged_model = merge_and_save_model(\n",
    "    base_model_id=model_id,\n",
    "    adapter_path=new_model,\n",
    "    output_path=f\"{new_model}_merged\"\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Model merging function ready (run when needed for deployment)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance benchmarking\n",
    "def benchmark_inference_speed(model, tokenizer, test_queries: List[str], num_runs: int = 3):\n",
    "    \"\"\"Benchmark inference speed and memory usage.\"\"\"\n",
    "    \n",
    "    print(\"Performance Benchmarking:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    times = []\n",
    "    memory_usage = []\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        run_times = []\n",
    "        \n",
    "        for query in test_queries:\n",
    "            # Clear cache\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Measure memory before\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "                start_memory = torch.cuda.memory_allocated()\n",
    "            \n",
    "            # Measure time\n",
    "            start_time = time.time()\n",
    "            response = generate_customer_support_response(query, max_length=256)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Measure memory after\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "                end_memory = torch.cuda.memory_allocated()\n",
    "                memory_usage.append((end_memory - start_memory) / 1024**2)  # MB\n",
    "            \n",
    "            run_times.append(end_time - start_time)\n",
    "        \n",
    "        times.extend(run_times)\n",
    "        print(f\"Run {run + 1} completed\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    avg_memory = np.mean(memory_usage) if memory_usage else 0\n",
    "    \n",
    "    print(f\"\\nResults (averaged over {len(times)} generations):\")\n",
    "    print(f\"Average inference time: {avg_time:.2f} Â± {std_time:.2f} seconds\")\n",
    "    print(f\"Tokens per second: ~{256 / avg_time:.1f}\")\n",
    "    if memory_usage:\n",
    "        print(f\"Average memory per inference: {avg_memory:.1f} MB\")\n",
    "    \n",
    "    return {\n",
    "        \"avg_time\": avg_time,\n",
    "        \"std_time\": std_time,\n",
    "        \"avg_memory\": avg_memory,\n",
    "        \"tokens_per_second\": 256 / avg_time\n",
    "    }\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_results = benchmark_inference_speed(model, tokenizer, test_queries[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory efficiency analysis\n",
    "def analyze_memory_efficiency():\n",
    "    \"\"\"Analyze memory efficiency compared to alternatives.\"\"\"\n",
    "    \n",
    "    print(\"Memory Efficiency Analysis:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Theoretical memory requirements (Llama 3.1 8B)\n",
    "    param_count = 8_000_000_000\n",
    "    \n",
    "    memory_requirements = {\n",
    "        \"Full Fine-tuning (FP32)\": param_count * 4 * 4,  # 4 bytes * 4 (model + gradients + optimizer states)\n",
    "        \"Full Fine-tuning (FP16)\": param_count * 2 * 4,  # 2 bytes * 4\n",
    "        \"LoRA (FP16)\": param_count * 2 + 50_000_000 * 2 * 4,  # Base model + LoRA adapters\n",
    "        \"QLoRA (NF4)\": param_count * 0.5 + 50_000_000 * 2 * 4,  # 4-bit base + 16-bit adapters\n",
    "    }\n",
    "    \n",
    "    print(\"Estimated Memory Requirements (8B model):\")\n",
    "    baseline = memory_requirements[\"Full Fine-tuning (FP32)\"]\n",
    "    \n",
    "    for method, memory in memory_requirements.items():\n",
    "        memory_gb = memory / 1024**3\n",
    "        reduction = (1 - memory / baseline) * 100\n",
    "        print(f\"{method:25}: {memory_gb:6.1f} GB ({reduction:+5.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nQLoRA enables training on GPUs with {memory_requirements['QLoRA (NF4)'] / 1024**3:.0f}GB+ VRAM\")\n",
    "    print(f\"This makes 65B models trainable on consumer GPUs!\")\n",
    "\n",
    "analyze_memory_efficiency()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Production Deployment Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production deployment utilities\n",
    "def create_deployment_config():\n",
    "    \"\"\"Create configuration for production deployment.\"\"\"\n",
    "    \n",
    "    deployment_config = {\n",
    "        \"model_optimization\": {\n",
    "            \"quantization\": \"4-bit NF4\",\n",
    "            \"adapter_format\": \"LoRA\",\n",
    "            \"precision\": \"bfloat16\",\n",
    "            \"compile\": True,  # Use torch.compile for speed\n",
    "        },\n",
    "        \n",
    "        \"inference_config\": {\n",
    "            \"max_new_tokens\": 512,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"repetition_penalty\": 1.1,\n",
    "            \"do_sample\": True,\n",
    "        },\n",
    "        \n",
    "        \"hardware_requirements\": {\n",
    "            \"min_gpu_memory\": \"8GB\",\n",
    "            \"recommended_gpu\": \"RTX 4090, A100, H100\",\n",
    "            \"cpu_cores\": 8,\n",
    "            \"ram\": \"32GB\",\n",
    "        },\n",
    "        \n",
    "        \"scaling\": {\n",
    "            \"batch_size\": 1,  # For real-time responses\n",
    "            \"concurrent_requests\": 4,\n",
    "            \"load_balancing\": \"round_robin\",\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return deployment_config\n",
    "\n",
    "# Production inference pipeline\n",
    "class ProductionQLoRAInference:\n",
    "    \"\"\"Production-ready QLoRA inference pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, base_model_id: str):\n",
    "        self.model_path = model_path\n",
    "        self.base_model_id = base_model_id\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.load_model()\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the model with optimizations.\"\"\"\n",
    "        print(\"Loading production model...\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_id)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Load quantized base model\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.base_model_id,\n",
    "            quantization_config=create_bnb_config(),\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        \n",
    "        # Load adapter\n",
    "        self.model = PeftModel.from_pretrained(base_model, self.model_path)\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(\"âœ… Production model loaded\")\n",
    "    \n",
    "    def generate_response(self, query: str, **kwargs) -> Dict:\n",
    "        \"\"\"Generate response with metadata.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Default generation parameters\n",
    "        gen_params = {\n",
    "            \"max_new_tokens\": 512,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"do_sample\": True,\n",
    "        }\n",
    "        gen_params.update(kwargs)\n",
    "        \n",
    "        # Generate response\n",
    "        response = generate_customer_support_response(query, gen_params[\"max_new_tokens\"])\n",
    "        \n",
    "        generation_time = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            \"response\": response,\n",
    "            \"generation_time\": generation_time,\n",
    "            \"parameters\": gen_params,\n",
    "            \"model_info\": {\n",
    "                \"base_model\": self.base_model_id,\n",
    "                \"adapter\": self.model_path,\n",
    "                \"quantization\": \"4-bit NF4\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "config = create_deployment_config()\n",
    "print(\"Production Deployment Configuration:\")\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "### QLoRA Advantages:\n",
    "1. **Memory Efficiency**: 75% reduction in GPU memory usage\n",
    "2. **Performance**: 99%+ of full fine-tuning quality\n",
    "3. **Accessibility**: Enable large model training on consumer hardware\n",
    "4. **Speed**: Faster training and inference compared to full fine-tuning\n",
    "\n",
    "### Best Practices:\n",
    "1. **Use NF4 quantization** for optimal quality-memory trade-off\n",
    "2. **Enable double quantization** for additional memory savings\n",
    "3. **Use paged optimizers** to handle memory spikes\n",
    "4. **Start with rank=16-64** for most tasks\n",
    "5. **Use bfloat16** for better numerical stability with quantization\n",
    "\n",
    "### Production Considerations:\n",
    "1. **Merge adapters** for deployment if memory allows\n",
    "2. **Use torch.compile** for inference optimization\n",
    "3. **Monitor memory usage** in production\n",
    "4. **Implement proper error handling** for OOM scenarios\n",
    "\n",
    "### When to Use QLoRA:\n",
    "- âœ… Limited GPU memory (8-16GB)\n",
    "- âœ… Training large models (7B+ parameters)\n",
    "- âœ… Rapid prototyping and experimentation\n",
    "- âœ… Cost-sensitive deployments\n",
    "\n",
    "### Limitations:\n",
    "- Slightly slower than merged models in inference\n",
    "- Requires careful memory management\n",
    "- Limited by adapter rank for complex adaptations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ‰ QLoRA Fine-tuning Complete!\")\n",
    "print(\"\\nKey Achievements:\")\n",
    "print(\"âœ… Successfully fine-tuned Llama 3.1 8B with 4-bit quantization\")\n",
    "print(\"âœ… Reduced memory usage by ~75% compared to full fine-tuning\")\n",
    "print(\"âœ… Created a customer support chatbot with domain-specific responses\")\n",
    "print(\"âœ… Demonstrated production deployment strategies\")\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Experiment with different LoRA configurations\")\n",
    "print(\"2. Test on larger datasets for your specific domain\")\n",
    "print(\"3. Implement production serving with proper monitoring\")\n",
    "print(\"4. Consider model merging for deployment optimization\")\n",
    "\n",
    "# Final memory usage\n",
    "print_memory_usage(\"Final\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
