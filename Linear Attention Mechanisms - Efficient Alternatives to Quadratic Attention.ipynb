{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Attention Mechanisms: Efficient Alternatives to Quadratic Attention\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook explores **Linear Attention Mechanisms** - efficient alternatives to the standard quadratic attention found in Transformers. We'll implement and compare various approaches including Performer, LinFormer, and other linear attention variants.\n",
    "\n",
    "### Key Topics Covered:\n",
    "1. **Background**: Understanding the quadratic complexity problem\n",
    "2. **Mathematical Foundations**: How linear attention works\n",
    "3. **Implementations**: Performer, LinFormer, and other variants\n",
    "4. **Comparisons**: Attention quality vs computational complexity\n",
    "5. **Scaling Analysis**: Performance characteristics\n",
    "6. **Visualizations**: Attention pattern analysis\n",
    "\n",
    "### Trade-offs We'll Explore:\n",
    "- **Attention Quality** vs **Computational Complexity**\n",
    "- **Memory Usage** vs **Approximation Accuracy**\n",
    "- **Training Speed** vs **Model Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete!\n",
      "PyTorch version: 2.7.1+cu126\n",
      "Device available: CUDA\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import math\n",
    "from typing import Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background: The Quadratic Attention Problem\n",
    "\n",
    "### Standard Self-Attention\n",
    "\n",
    "The standard self-attention mechanism in Transformers computes:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- $Q \\in \\mathbb{R}^{n \\times d_k}$ (queries)\n",
    "- $K \\in \\mathbb{R}^{n \\times d_k}$ (keys)  \n",
    "- $V \\in \\mathbb{R}^{n \\times d_v}$ (values)\n",
    "- $n$ is the sequence length\n",
    "\n",
    "### The Problem: O(n²) Complexity\n",
    "\n",
    "Computing $QK^T$ requires $O(n^2 d_k)$ operations, making it prohibitive for long sequences.\n",
    "\n",
    "### Memory Usage\n",
    "The attention matrix requires $O(n^2)$ memory, which becomes massive for long sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Attention Complexity Analysis:\n",
      "Seq Length | Time (s) | Memory (MB)\n",
      "----------------------------------------\n",
      "     128 |   4.14ms |     0.06MB\n",
      "     256 |   6.19ms |     0.25MB\n",
      "     512 |  23.08ms |     1.00MB\n",
      "    1024 |  99.74ms |     4.00MB\n",
      "    2048 | 337.77ms |    16.00MB\n"
     ]
    }
   ],
   "source": [
    "class StandardAttention(nn.Module):\n",
    "    \"\"\"Standard O(n²) self-attention mechanism.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.d_k)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.w_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Attention computation - O(n²) complexity\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # Reshape and project\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        out = self.w_o(out)\n",
    "        \n",
    "        return out, attention_weights\n",
    "\n",
    "# Demonstrate complexity\n",
    "def analyze_complexity(seq_lengths, d_model=512):\n",
    "    \"\"\"Analyze time and memory complexity of standard attention.\"\"\"\n",
    "    results = {'seq_len': [], 'time': [], 'memory': []}\n",
    "    \n",
    "    attention = StandardAttention(d_model)\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        x = torch.randn(1, seq_len, d_model)\n",
    "        \n",
    "        # Measure time\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            out, weights = attention(x)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate theoretical memory for attention matrix\n",
    "        attention_memory = seq_len ** 2 * 4  # 4 bytes per float32\n",
    "        \n",
    "        results['seq_len'].append(seq_len)\n",
    "        results['time'].append(end_time - start_time)\n",
    "        results['memory'].append(attention_memory)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze complexity for different sequence lengths\n",
    "seq_lengths = [128, 256, 512, 1024, 2048]\n",
    "complexity_results = analyze_complexity(seq_lengths)\n",
    "\n",
    "print(\"Standard Attention Complexity Analysis:\")\n",
    "print(\"Seq Length | Time (s) | Memory (MB)\")\n",
    "print(\"-\" * 40)\n",
    "for i, seq_len in enumerate(complexity_results['seq_len']):\n",
    "    time_ms = complexity_results['time'][i] * 1000\n",
    "    memory_mb = complexity_results['memory'][i] / (1024 * 1024)\n",
    "    print(f\"{seq_len:8d} | {time_ms:6.2f}ms | {memory_mb:8.2f}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear Attention: Mathematical Foundation\n",
    "\n",
    "### Key Insight: Kernel Trick\n",
    "\n",
    "Linear attention methods reformulate attention using the **kernel trick**:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\phi(Q) \\left( \\phi(K)^T V \\right)$$\n",
    "\n",
    "Where $\\phi: \\mathbb{R}^{d_k} \\rightarrow \\mathbb{R}^{d_\\phi}$ is a feature mapping function.\n",
    "\n",
    "### Complexity Reduction\n",
    "\n",
    "Instead of computing $O(n^2)$ attention matrix:\n",
    "1. Compute $\\phi(K)^T V$ first: $O(n d_\\phi d_v)$\n",
    "2. Then multiply by $\\phi(Q)$: $O(n d_\\phi d_v)$\n",
    "\n",
    "**Total complexity: $O(n d_\\phi d_v)$ - Linear in sequence length!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear attention base class defined!\n",
      "Key insight: O(n²) → O(n) complexity through kernel trick\n"
     ]
    }
   ],
   "source": [
    "class LinearAttentionBase(nn.Module):\n",
    "    \"\"\"Base class for linear attention mechanisms.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int = 8, feature_dim: int = 256):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.feature_dim = feature_dim\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def feature_map(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Feature mapping function φ(x). To be implemented by subclasses.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.w_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Apply feature mapping\n",
    "        Q_feat = self.feature_map(Q)  # [batch, heads, seq_len, feature_dim]\n",
    "        K_feat = self.feature_map(K)  # [batch, heads, seq_len, feature_dim]\n",
    "        \n",
    "        # Linear attention computation: φ(Q) * (φ(K)^T * V)\n",
    "        # Step 1: Compute φ(K)^T * V\n",
    "        KV = torch.matmul(K_feat.transpose(-2, -1), V)  # [batch, heads, feature_dim, d_k]\n",
    "        \n",
    "        # Step 2: Compute φ(Q) * (φ(K)^T * V)\n",
    "        out = torch.matmul(Q_feat, KV)  # [batch, heads, seq_len, d_k]\n",
    "        \n",
    "        # Normalization (important for stable training)\n",
    "        normalizer = torch.matmul(Q_feat, K_feat.sum(dim=-2, keepdim=True).transpose(-2, -1))\n",
    "        out = out / (normalizer + 1e-8)\n",
    "        \n",
    "        # Reshape and project\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        out = self.w_o(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "print(\"Linear attention base class defined!\")\n",
    "print(\"Key insight: O(n²) → O(n) complexity through kernel trick\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performer: Random Feature Attention\n",
    "\n",
    "### The Performer Approach\n",
    "\n",
    "**Paper**: \"Rethinking Attention with Performers\" (Choromanski et al., 2020)\n",
    "\n",
    "### Key Innovation: FAVOR+ Algorithm\n",
    "\n",
    "Performer approximates the softmax kernel using random Fourier features:\n",
    "\n",
    "$$\\phi(x) = \\frac{1}{\\sqrt{m}} \\left[ \\exp(w_1^T x), \\exp(w_2^T x), \\ldots, \\exp(w_m^T x) \\right]$$\n",
    "\n",
    "Where $w_i \\sim \\mathcal{N}(0, I)$ are random Gaussian vectors.\n",
    "\n",
    "### Mathematical Properties:\n",
    "- **Unbiased**: $\\mathbb{E}[\\phi(q)^T \\phi(k)] = \\exp(q^T k)$\n",
    "- **Positive**: All features are positive (crucial for attention)\n",
    "- **Fast**: Linear complexity in sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Performer Attention:\n",
      "Input shape: torch.Size([2, 1024, 512])\n",
      "Standard Attention: 0.1759s\n",
      "Performer: 0.0294s (6.0x speedup)\n",
      "Performer (Orthogonal): 0.0111s (15.9x speedup)\n",
      "\n",
      "Output similarity (MSE with standard):\n",
      "Performer: 0.004882\n",
      "Performer (Orthogonal): 0.001634\n"
     ]
    }
   ],
   "source": [
    "class PerformerAttention(LinearAttentionBase):\n",
    "    \"\"\"Performer attention using FAVOR+ algorithm with random features.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int = 8, feature_dim: int = 256, \n",
    "                 redraw_features: bool = True):\n",
    "        super().__init__(d_model, n_heads, feature_dim)\n",
    "        self.redraw_features = redraw_features\n",
    "        self.register_buffer('random_features', None)\n",
    "        self._create_projection_matrix()\n",
    "    \n",
    "    def _create_projection_matrix(self):\n",
    "        \"\"\"Create random projection matrix for FAVOR+.\"\"\"\n",
    "        # Random Gaussian matrix\n",
    "        random_matrix = torch.randn(self.feature_dim, self.d_k)\n",
    "        \n",
    "        # Optional: Use structured random matrices (faster)\n",
    "        # This is an advanced optimization from the paper\n",
    "        self.register_buffer('projection_matrix', random_matrix)\n",
    "    \n",
    "    def feature_map(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"FAVOR+ feature mapping with positive random features.\"\"\"\n",
    "        # x shape: [batch, heads, seq_len, d_k]\n",
    "        \n",
    "        # Project to random features\n",
    "        projected = torch.matmul(x, self.projection_matrix.T)  # [batch, heads, seq_len, feature_dim]\n",
    "        \n",
    "        # Apply nonlinearity: exp(proj - max) for numerical stability\n",
    "        max_proj = torch.max(projected, dim=-1, keepdim=True)[0]\n",
    "        projected = projected - max_proj\n",
    "        \n",
    "        # Positive random features\n",
    "        features = torch.exp(projected)\n",
    "        \n",
    "        # Normalization for unbiased estimation\n",
    "        norm_factor = math.sqrt(self.feature_dim)\n",
    "        features = features / norm_factor\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def redraw_projection_matrix(self):\n",
    "        \"\"\"Redraw random features (useful during training).\"\"\"\n",
    "        self._create_projection_matrix()\n",
    "\n",
    "\n",
    "class PerformerWithOrthogonal(PerformerAttention):\n",
    "    \"\"\"Enhanced Performer with orthogonal random features for better approximation.\"\"\"\n",
    "    \n",
    "    def _create_projection_matrix(self):\n",
    "        \"\"\"Create orthogonal random projection matrix.\"\"\"\n",
    "        # Generate random matrix\n",
    "        random_matrix = torch.randn(self.feature_dim, self.d_k)\n",
    "        \n",
    "        # Make it orthogonal using QR decomposition\n",
    "        if self.feature_dim >= self.d_k:\n",
    "            q, _ = torch.qr(random_matrix.T)\n",
    "            random_matrix = q.T[:self.feature_dim]\n",
    "        \n",
    "        self.register_buffer('projection_matrix', random_matrix)\n",
    "\n",
    "\n",
    "# Test Performer implementation\n",
    "def test_performer():\n",
    "    \"\"\"Test Performer attention implementation.\"\"\"\n",
    "    d_model = 512\n",
    "    seq_len = 1024\n",
    "    batch_size = 2\n",
    "    \n",
    "    # Create models\n",
    "    standard_attn = StandardAttention(d_model)\n",
    "    performer_attn = PerformerAttention(d_model, feature_dim=256)\n",
    "    performer_ortho = PerformerWithOrthogonal(d_model, feature_dim=256)\n",
    "    \n",
    "    # Test input\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    print(\"Testing Performer Attention:\")\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    \n",
    "    # Time standard attention\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        out_std, weights_std = standard_attn(x)\n",
    "    std_time = time.time() - start_time\n",
    "    \n",
    "    # Time Performer\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        out_perf = performer_attn(x)\n",
    "    perf_time = time.time() - start_time\n",
    "    \n",
    "    # Time Orthogonal Performer\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        out_ortho = performer_ortho(x)\n",
    "    ortho_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Standard Attention: {std_time:.4f}s\")\n",
    "    print(f\"Performer: {perf_time:.4f}s ({std_time/perf_time:.1f}x speedup)\")\n",
    "    print(f\"Performer (Orthogonal): {ortho_time:.4f}s ({std_time/ortho_time:.1f}x speedup)\")\n",
    "    \n",
    "    # Check output similarity\n",
    "    mse_perf = F.mse_loss(out_std, out_perf)\n",
    "    mse_ortho = F.mse_loss(out_std, out_ortho)\n",
    "    \n",
    "    print(f\"\\nOutput similarity (MSE with standard):\")\n",
    "    print(f\"Performer: {mse_perf:.6f}\")\n",
    "    print(f\"Performer (Orthogonal): {mse_ortho:.6f}\")\n",
    "\n",
    "test_performer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LinFormer: Low-Rank Attention\n",
    "\n",
    "### The LinFormer Approach\n",
    "\n",
    "**Paper**: \"Linformer: Self-Attention with Linear Complexity\" (Wang et al., 2020)\n",
    "\n",
    "### Key Innovation: Projected Keys and Values\n",
    "\n",
    "LinFormer reduces attention complexity by projecting keys and values to a lower dimension:\n",
    "\n",
    "$$\\text{LinFormer}(Q, K, V) = \\text{softmax}\\left(\\frac{Q(PK)^T}{\\sqrt{d_k}}\\right)(PV)$$\n",
    "\n",
    "Where $P \\in \\mathbb{R}^{k \\times n}$ is a projection matrix with $k \\ll n$.\n",
    "\n",
    "### Complexity Analysis:\n",
    "- **Standard**: $O(n^2 d)$\n",
    "- **LinFormer**: $O(nkd)$ where $k \\ll n$\n",
    "- **Memory**: Reduced from $O(n^2)$ to $O(nk)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinFormer Complexity Analysis:\n",
      "Seq Len | Proj Dim | Standard Time | LinFormer Time | Speedup\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1024x256 and 64x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 132\u001b[0m\n\u001b[1;32m    129\u001b[0m         mse \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(out_std, out_lin)\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        | MSE vs Standard: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 132\u001b[0m \u001b[43mtest_linformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 121\u001b[0m, in \u001b[0;36mtest_linformer\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 121\u001b[0m     out_lin \u001b[38;5;241m=\u001b[39m \u001b[43mlinformer_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m lin_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    124\u001b[0m speedup \u001b[38;5;241m=\u001b[39m std_time \u001b[38;5;241m/\u001b[39m lin_time \u001b[38;5;28;01mif\u001b[39;00m lin_time \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 43\u001b[0m, in \u001b[0;36mLinFormerAttention.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Project keys and values to lower dimension\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# Apply projection along sequence dimension\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     K_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch, heads, projected_dim, d_k]\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     V_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_v(V\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch, heads, projected_dim, d_k]\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconv\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# Reshape for conv1d\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1024x256 and 64x64)"
     ]
    }
   ],
   "source": [
    "class LinFormerAttention(nn.Module):\n",
    "    \"\"\"LinFormer attention with linear complexity through low-rank projection.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int = 8, projected_dim: int = 256, \n",
    "                 projection_type: str = 'linear'):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.projected_dim = projected_dim\n",
    "        self.projection_type = projection_type\n",
    "        \n",
    "        # Standard Q, K, V projections\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Projection matrices for keys and values\n",
    "        if projection_type == 'linear':\n",
    "            self.proj_k = nn.Linear(self.d_k, projected_dim, bias=False)\n",
    "            self.proj_v = nn.Linear(self.d_k, projected_dim, bias=False)\n",
    "        elif projection_type == 'conv':\n",
    "            self.proj_k = nn.Conv1d(self.d_k, projected_dim, 1, bias=False)\n",
    "            self.proj_v = nn.Conv1d(self.d_k, projected_dim, 1, bias=False)\n",
    "        \n",
    "        self.scale = math.sqrt(self.d_k)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.w_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Project keys and values to lower dimension\n",
    "        if self.projection_type == 'linear':\n",
    "            # Apply projection along sequence dimension\n",
    "            K_proj = self.proj_k(K.transpose(-2, -1)).transpose(-2, -1)  # [batch, heads, projected_dim, d_k]\n",
    "            V_proj = self.proj_v(V.transpose(-2, -1)).transpose(-2, -1)  # [batch, heads, projected_dim, d_k]\n",
    "        elif self.projection_type == 'conv':\n",
    "            # Reshape for conv1d\n",
    "            K_reshaped = K.view(-1, self.d_k, seq_len)\n",
    "            V_reshaped = V.view(-1, self.d_k, seq_len)\n",
    "            \n",
    "            K_proj = self.proj_k(K_reshaped).view(batch_size, self.n_heads, self.projected_dim, -1).transpose(-2, -1)\n",
    "            V_proj = self.proj_v(V_reshaped).view(batch_size, self.n_heads, self.projected_dim, -1).transpose(-2, -1)\n",
    "        \n",
    "        # Attention computation with reduced complexity\n",
    "        scores = torch.matmul(Q, K_proj.transpose(-2, -1)) / self.scale  # [batch, heads, seq_len, projected_dim]\n",
    "        \n",
    "        if mask is not None:\n",
    "            # Adapt mask for projected dimension\n",
    "            projected_mask = mask[:, :self.projected_dim] if mask.size(-1) > self.projected_dim else mask\n",
    "            scores = scores.masked_fill(projected_mask.unsqueeze(1).unsqueeze(1) == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention to projected values\n",
    "        out = torch.matmul(attention_weights, V_proj)  # [batch, heads, seq_len, d_k]\n",
    "        \n",
    "        # Reshape and project\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        out = self.w_o(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class AdaptiveLinFormer(LinFormerAttention):\n",
    "    \"\"\"LinFormer with adaptive projection dimension based on sequence length.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int = 8, max_projected_dim: int = 512,\n",
    "                 compression_ratio: float = 0.25):\n",
    "        # Start with max dimension, will be adjusted in forward pass\n",
    "        super().__init__(d_model, n_heads, max_projected_dim)\n",
    "        self.max_projected_dim = max_projected_dim\n",
    "        self.compression_ratio = compression_ratio\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        seq_len = x.size(1)\n",
    "        # Adaptive projection dimension\n",
    "        self.projected_dim = min(self.max_projected_dim, int(seq_len * self.compression_ratio))\n",
    "        self.projected_dim = max(self.projected_dim, 64)  # Minimum dimension\n",
    "        \n",
    "        return super().forward(x, mask)\n",
    "\n",
    "\n",
    "# Test LinFormer implementation\n",
    "def test_linformer():\n",
    "    \"\"\"Test LinFormer attention implementation.\"\"\"\n",
    "    d_model = 512\n",
    "    batch_size = 2\n",
    "    \n",
    "    # Test different sequence lengths\n",
    "    seq_lengths = [256, 512, 1024, 2048]\n",
    "    \n",
    "    print(\"LinFormer Complexity Analysis:\")\n",
    "    print(\"Seq Len | Proj Dim | Standard Time | LinFormer Time | Speedup\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        x = torch.randn(batch_size, seq_len, d_model)\n",
    "        projected_dim = min(256, seq_len // 4)  # 25% compression\n",
    "        \n",
    "        # Standard attention\n",
    "        standard_attn = StandardAttention(d_model)\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            out_std, _ = standard_attn(x)\n",
    "        std_time = time.time() - start_time\n",
    "        \n",
    "        # LinFormer attention\n",
    "        linformer_attn = LinFormerAttention(d_model, projected_dim=projected_dim)\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            out_lin = linformer_attn(x)\n",
    "        lin_time = time.time() - start_time\n",
    "        \n",
    "        speedup = std_time / lin_time if lin_time > 0 else float('inf')\n",
    "        \n",
    "        print(f\"{seq_len:7d} | {projected_dim:8d} | {std_time:11.4f}s | {lin_time:12.4f}s | {speedup:6.1f}x\")\n",
    "        \n",
    "        # Check output similarity\n",
    "        mse = F.mse_loss(out_std, out_lin)\n",
    "        print(f\"        | MSE vs Standard: {mse:.6f}\")\n",
    "\n",
    "test_linformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Other Linear Attention Variants\n",
    "\n",
    "### FNet: Fourier Transform Attention\n",
    "Replaces attention entirely with FFT operations.\n",
    "\n",
    "### Synthesizer: Learned Synthetic Attention\n",
    "Learns attention patterns without content-based interactions.\n",
    "\n",
    "### Linear Transformer: Causal Linear Attention\n",
    "Efficient linear attention for autoregressive tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNetBlock(nn.Module):\n",
    "    \"\"\"FNet: Replace attention with 2D Fourier Transform.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Apply 2D FFT\n",
    "        # x shape: [batch, seq_len, d_model]\n",
    "        x_fft = torch.fft.fft2(x.float())\n",
    "        x_real = x_fft.real\n",
    "        \n",
    "        return self.norm(x_real)\n",
    "\n",
    "\n",
    "class SynthesizerAttention(nn.Module):\n",
    "    \"\"\"Synthesizer: Learned synthetic attention patterns.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int = 8, max_seq_len: int = 2048,\n",
    "                 synthesizer_type: str = 'dense'):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.synthesizer_type = synthesizer_type\n",
    "        \n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        if synthesizer_type == 'dense':\n",
    "            # Dense synthesizer: learned attention weights\n",
    "            self.synthetic_attn = nn.Parameter(\n",
    "                torch.randn(n_heads, max_seq_len, max_seq_len) * 0.02\n",
    "            )\n",
    "        elif synthesizer_type == 'random':\n",
    "            # Random synthesizer: fixed random attention\n",
    "            self.register_buffer(\n",
    "                'synthetic_attn',\n",
    "                torch.randn(n_heads, max_seq_len, max_seq_len)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Only compute values (no queries or keys needed!)\n",
    "        V = self.w_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Use synthetic attention patterns\n",
    "        attn_weights = self.synthetic_attn[:, :seq_len, :seq_len]\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        \n",
    "        # Apply synthetic attention\n",
    "        out = torch.matmul(attn_weights.unsqueeze(0), V)  # Broadcast across batch\n",
    "        \n",
    "        # Reshape and project\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        out = self.w_o(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class LinearTransformerAttention(LinearAttentionBase):\n",
    "    \"\"\"Linear Transformer with causal masking for autoregressive tasks.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int = 8, feature_dim: int = 256,\n",
    "                 causal: bool = True):\n",
    "        super().__init__(d_model, n_heads, feature_dim)\n",
    "        self.causal = causal\n",
    "    \n",
    "    def feature_map(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"ELU-based feature mapping for positive features.\"\"\"\n",
    "        return F.elu(x) + 1\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if not self.causal:\n",
    "            return super().forward(x)\n",
    "        \n",
    "        # Causal version using cumulative sums\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.w_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Apply feature mapping\n",
    "        Q_feat = self.feature_map(Q)\n",
    "        K_feat = self.feature_map(K)\n",
    "        \n",
    "        # Causal linear attention using cumulative sums\n",
    "        KV = K_feat.unsqueeze(-1) * V.unsqueeze(-2)  # Outer product\n",
    "        KV_cumsum = torch.cumsum(KV, dim=2)  # Cumulative sum over sequence\n",
    "        \n",
    "        # Compute output\n",
    "        out = torch.sum(Q_feat.unsqueeze(-1) * KV_cumsum, dim=-2)\n",
    "        \n",
    "        # Normalization\n",
    "        normalizer = torch.cumsum(K_feat, dim=2)\n",
    "        normalizer = torch.sum(Q_feat * normalizer, dim=-1, keepdim=True)\n",
    "        out = out / (normalizer + 1e-8)\n",
    "        \n",
    "        # Reshape and project\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        out = self.w_o(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# Test all variants\n",
    "def test_all_variants():\n",
    "    \"\"\"Test all linear attention variants.\"\"\"\n",
    "    d_model = 512\n",
    "    seq_len = 1024\n",
    "    batch_size = 2\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    models = {\n",
    "        'FNet': FNetBlock(d_model),\n",
    "        'Synthesizer (Dense)': SynthesizerAttention(d_model, synthesizer_type='dense'),\n",
    "        'Synthesizer (Random)': SynthesizerAttention(d_model, synthesizer_type='random'),\n",
    "        'Linear Transformer': LinearTransformerAttention(d_model, causal=True),\n",
    "    }\n",
    "    \n",
    "    print(\"Testing Alternative Linear Attention Variants:\")\n",
    "    print(\"Model\\t\\t\\t| Time (ms) | Output Shape\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            output = model(x)\n",
    "        elapsed = (time.time() - start_time) * 1000\n",
    "        \n",
    "        print(f\"{name:20s} | {elapsed:7.2f}ms | {tuple(output.shape)}\")\n",
    "\n",
    "test_all_variants()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comprehensive Comparison and Analysis\n",
    "\n",
    "### Attention Quality vs Computational Complexity\n",
    "\n",
    "Let's systematically compare all attention mechanisms across multiple dimensions:\n",
    "1. **Computational Complexity**\n",
    "2. **Memory Usage**\n",
    "3. **Attention Quality (approximation error)**\n",
    "4. **Scaling Behavior**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def comprehensive_benchmark():\n",
    "    \"\"\"Comprehensive benchmark of all attention mechanisms.\"\"\"\n",
    "    \n",
    "    d_model = 512\n",
    "    batch_size = 4\n",
    "    seq_lengths = [128, 256, 512, 1024, 2048]\n",
    "    \n",
    "    # Initialize models\n",
    "    models = {\n",
    "        'Standard': StandardAttention(d_model),\n",
    "        'Performer': PerformerAttention(d_model, feature_dim=256),\n",
    "        'Performer (Ortho)': PerformerWithOrthogonal(d_model, feature_dim=256),\n",
    "        'LinFormer': LinFormerAttention(d_model, projected_dim=256),\n",
    "        'Linear Transformer': LinearTransformerAttention(d_model, causal=False),\n",
    "        'FNet': FNetBlock(d_model),\n",
    "    }\n",
    "    \n",
    "    results = defaultdict(list)\n",
    "    \n",
    "    print(\"Running comprehensive benchmark...\")\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        print(f\"\\nTesting sequence length: {seq_len}\")\n",
    "        x = torch.randn(batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Get standard attention output as reference\n",
    "        with torch.no_grad():\n",
    "            if seq_len <= 1024:  # Only compute for manageable sizes\n",
    "                ref_output, ref_weights = models['Standard'](x)\n",
    "            else:\n",
    "                ref_output = None\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            try:\n",
    "                # Measure time\n",
    "                times = []\n",
    "                for _ in range(5):  # Multiple runs for stability\n",
    "                    start_time = time.time()\n",
    "                    with torch.no_grad():\n",
    "                        if name == 'Standard':\n",
    "                            output, _ = model(x)\n",
    "                        else:\n",
    "                            output = model(x)\n",
    "                    times.append(time.time() - start_time)\n",
    "                \n",
    "                avg_time = np.mean(times) * 1000  # Convert to ms\n",
    "                \n",
    "                # Calculate approximation error\n",
    "                if ref_output is not None and name != 'Standard':\n",
    "                    mse_error = F.mse_loss(output, ref_output).item()\n",
    "                    cosine_sim = F.cosine_similarity(\n",
    "                        output.flatten(), ref_output.flatten(), dim=0\n",
    "                    ).item()\n",
    "                else:\n",
    "                    mse_error = 0.0 if name == 'Standard' else float('nan')\n",
    "                    cosine_sim = 1.0 if name == 'Standard' else float('nan')\n",
    "                \n",
    "                # Memory estimation (theoretical)\n",
    "                if name == 'Standard':\n",
    "                    memory_mb = (seq_len ** 2) * 4 / (1024 ** 2)  # Attention matrix\n",
    "                elif name in ['Performer', 'Performer (Ortho)']:\n",
    "                    memory_mb = seq_len * 256 * 4 / (1024 ** 2)  # Feature dimension\n",
    "                elif name == 'LinFormer':\n",
    "                    memory_mb = seq_len * 256 * 4 / (1024 ** 2)  # Projected dimension\n",
    "                else:\n",
    "                    memory_mb = seq_len * d_model * 4 / (1024 ** 2)  # Linear in seq_len\n",
    "                \n",
    "                # Store results\n",
    "                results['Model'].append(name)\n",
    "                results['Seq_Length'].append(seq_len)\n",
    "                results['Time_ms'].append(avg_time)\n",
    "                results['Memory_MB'].append(memory_mb)\n",
    "                results['MSE_Error'].append(mse_error)\n",
    "                results['Cosine_Sim'].append(cosine_sim)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error with {name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_df = comprehensive_benchmark()\n",
    "print(\"\\nBenchmark completed!\")\n",
    "print(f\"Total results: {len(benchmark_df)} measurements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of results\n",
    "def plot_comprehensive_analysis(df):\n",
    "    \"\"\"Create comprehensive visualizations of attention mechanism comparison.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Linear Attention Mechanisms: Comprehensive Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Time vs Sequence Length\n",
    "    ax1 = axes[0, 0]\n",
    "    for model in df['Model'].unique():\n",
    "        model_data = df[df['Model'] == model]\n",
    "        ax1.plot(model_data['Seq_Length'], model_data['Time_ms'], 'o-', label=model, linewidth=2, markersize=6)\n",
    "    ax1.set_xlabel('Sequence Length')\n",
    "    ax1.set_ylabel('Time (ms)')\n",
    "    ax1.set_title('Computational Time vs Sequence Length')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Memory vs Sequence Length\n",
    "    ax2 = axes[0, 1]\n",
    "    for model in df['Model'].unique():\n",
    "        model_data = df[df['Model'] == model]\n",
    "        ax2.plot(model_data['Seq_Length'], model_data['Memory_MB'], 's-', label=model, linewidth=2, markersize=6)\n",
    "    ax2.set_xlabel('Sequence Length')\n",
    "    ax2.set_ylabel('Memory (MB)')\n",
    "    ax2.set_title('Memory Usage vs Sequence Length')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.set_xscale('log')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Approximation Quality (MSE Error)\n",
    "    ax3 = axes[0, 2]\n",
    "    quality_df = df[df['Model'] != 'Standard'].dropna(subset=['MSE_Error'])\n",
    "    if not quality_df.empty:\n",
    "        for model in quality_df['Model'].unique():\n",
    "            model_data = quality_df[quality_df['Model'] == model]\n",
    "            ax3.plot(model_data['Seq_Length'], model_data['MSE_Error'], '^-', label=model, linewidth=2, markersize=6)\n",
    "        ax3.set_xlabel('Sequence Length')\n",
    "        ax3.set_ylabel('MSE Error vs Standard')\n",
    "        ax3.set_title('Approximation Quality (Lower is Better)')\n",
    "        ax3.set_yscale('log')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Cosine Similarity\n",
    "    ax4 = axes[1, 0]\n",
    "    similarity_df = df[df['Model'] != 'Standard'].dropna(subset=['Cosine_Sim'])\n",
    "    if not similarity_df.empty:\n",
    "        for model in similarity_df['Model'].unique():\n",
    "            model_data = similarity_df[similarity_df['Model'] == model]\n",
    "            ax4.plot(model_data['Seq_Length'], model_data['Cosine_Sim'], 'd-', label=model, linewidth=2, markersize=6)\n",
    "        ax4.set_xlabel('Sequence Length')\n",
    "        ax4.set_ylabel('Cosine Similarity with Standard')\n",
    "        ax4.set_title('Output Similarity (Higher is Better)')\n",
    "        ax4.set_ylim(0, 1.1)\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Time vs Quality Trade-off (at seq_len=1024)\n",
    "    ax5 = axes[1, 1]\n",
    "    tradeoff_df = df[(df['Seq_Length'] == 1024) & (df['Model'] != 'Standard')].dropna(subset=['MSE_Error'])\n",
    "    if not tradeoff_df.empty:\n",
    "        scatter = ax5.scatter(tradeoff_df['Time_ms'], tradeoff_df['MSE_Error'], \n",
    "                            s=100, alpha=0.7, c=range(len(tradeoff_df)), cmap='viridis')\n",
    "        for i, model in enumerate(tradeoff_df['Model']):\n",
    "            ax5.annotate(model, (tradeoff_df.iloc[i]['Time_ms'], tradeoff_df.iloc[i]['MSE_Error']),\n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "        ax5.set_xlabel('Time (ms)')\n",
    "        ax5.set_ylabel('MSE Error')\n",
    "        ax5.set_title('Speed vs Quality Trade-off (Seq Len=1024)')\n",
    "        ax5.set_yscale('log')\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Scaling Analysis (Time complexity)\n",
    "    ax6 = axes[1, 2]\n",
    "    \n",
    "    # Fit polynomial to estimate complexity\n",
    "    for model in df['Model'].unique():\n",
    "        model_data = df[df['Model'] == model].sort_values('Seq_Length')\n",
    "        if len(model_data) >= 3:\n",
    "            x = np.log(model_data['Seq_Length'].values)\n",
    "            y = np.log(model_data['Time_ms'].values)\n",
    "            coeffs = np.polyfit(x, y, 1)\n",
    "            complexity_order = coeffs[0]\n",
    "            \n",
    "            ax6.plot(model_data['Seq_Length'], model_data['Time_ms'], 'o-', \n",
    "                    label=f'{model} (O(n^{complexity_order:.1f}))', linewidth=2, markersize=6)\n",
    "    \n",
    "    ax6.set_xlabel('Sequence Length')\n",
    "    ax6.set_ylabel('Time (ms)')\n",
    "    ax6.set_title('Empirical Complexity Analysis')\n",
    "    ax6.set_yscale('log')\n",
    "    ax6.set_xscale('log')\n",
    "    ax6.legend()\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create visualizations\n",
    "plot_comprehensive_analysis(benchmark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics and insights\n",
    "def generate_insights(df):\n",
    "    \"\"\"Generate key insights from the benchmark results.\"\"\"\n",
    "    \n",
    "    print(\"📊 KEY INSIGHTS FROM LINEAR ATTENTION ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Speed Analysis\n",
    "    print(\"\\n🚀 SPEED ANALYSIS:\")\n",
    "    max_seq = df['Seq_Length'].max()\n",
    "    speed_data = df[df['Seq_Length'] == max_seq]\n",
    "    standard_time = speed_data[speed_data['Model'] == 'Standard']['Time_ms'].iloc[0]\n",
    "    \n",
    "    print(f\"At sequence length {max_seq}:\")\n",
    "    for _, row in speed_data.iterrows():\n",
    "        if row['Model'] != 'Standard':\n",
    "            speedup = standard_time / row['Time_ms']\n",
    "            print(f\"  • {row['Model']:20s}: {speedup:5.1f}x speedup\")\n",
    "    \n",
    "    # 2. Memory Analysis\n",
    "    print(\"\\n💾 MEMORY EFFICIENCY:\")\n",
    "    memory_data = df[df['Seq_Length'] == max_seq]\n",
    "    standard_memory = memory_data[memory_data['Model'] == 'Standard']['Memory_MB'].iloc[0]\n",
    "    \n",
    "    print(f\"Memory reduction at sequence length {max_seq}:\")\n",
    "    for _, row in memory_data.iterrows():\n",
    "        if row['Model'] != 'Standard':\n",
    "            reduction = standard_memory / row['Memory_MB']\n",
    "            print(f\"  • {row['Model']:20s}: {reduction:5.1f}x less memory\")\n",
    "    \n",
    "    # 3. Quality Analysis\n",
    "    print(\"\\n🎯 APPROXIMATION QUALITY:\")\n",
    "    quality_data = df[(df['Seq_Length'] == 1024) & (df['Model'] != 'Standard')].dropna(subset=['MSE_Error'])\n",
    "    \n",
    "    if not quality_data.empty:\n",
    "        print(\"MSE Error vs Standard Attention (at seq_len=1024):\")\n",
    "        for _, row in quality_data.sort_values('MSE_Error').iterrows():\n",
    "            print(f\"  • {row['Model']:20s}: {row['MSE_Error']:.6f}\")\n",
    "        \n",
    "        print(\"\\nCosine Similarity with Standard (at seq_len=1024):\")\n",
    "        for _, row in quality_data.sort_values('Cosine_Sim', ascending=False).iterrows():\n",
    "            print(f\"  • {row['Model']:20s}: {row['Cosine_Sim']:.4f}\")\n",
    "    \n",
    "    # 4. Scaling Analysis\n",
    "    print(\"\\n📈 SCALING BEHAVIOR:\")\n",
    "    print(\"Theoretical vs Empirical complexity:\")\n",
    "    \n",
    "    complexity_theory = {\n",
    "        'Standard': 'O(n²)',\n",
    "        'Performer': 'O(n)',\n",
    "        'Performer (Ortho)': 'O(n)',\n",
    "        'LinFormer': 'O(n)',\n",
    "        'Linear Transformer': 'O(n)',\n",
    "        'FNet': 'O(n log n)'\n",
    "    }\n",
    "    \n",
    "    for model in df['Model'].unique():\n",
    "        model_data = df[df['Model'] == model].sort_values('Seq_Length')\n",
    "        if len(model_data) >= 3:\n",
    "            x = np.log(model_data['Seq_Length'].values)\n",
    "            y = np.log(model_data['Time_ms'].values)\n",
    "            coeffs = np.polyfit(x, y, 1)\n",
    "            empirical_order = coeffs[0]\n",
    "            theoretical = complexity_theory.get(model, 'Unknown')\n",
    "            print(f\"  • {model:20s}: {theoretical:8s} → O(n^{empirical_order:.1f}) empirical\")\n",
    "    \n",
    "    # 5. Recommendations\n",
    "    print(\"\\n✅ RECOMMENDATIONS:\")\n",
    "    print(\"\\n🎯 For SPEED priority:\")\n",
    "    fastest = speed_data.loc[speed_data[speed_data['Model'] != 'Standard']['Time_ms'].idxmin()]\n",
    "    print(f\"  → Use {fastest['Model']} (fastest linear attention)\")\n",
    "    \n",
    "    print(\"\\n🎯 For QUALITY priority:\")\n",
    "    if not quality_data.empty:\n",
    "        best_quality = quality_data.loc[quality_data['MSE_Error'].idxmin()]\n",
    "        print(f\"  → Use {best_quality['Model']} (best approximation quality)\")\n",
    "    \n",
    "    print(\"\\n🎯 For BALANCED performance:\")\n",
    "    if not quality_data.empty:\n",
    "        # Normalize metrics and compute composite score\n",
    "        normalized_time = quality_data['Time_ms'] / quality_data['Time_ms'].max()\n",
    "        normalized_error = quality_data['MSE_Error'] / quality_data['MSE_Error'].max()\n",
    "        composite_score = normalized_time + normalized_error  # Lower is better\n",
    "        balanced_idx = composite_score.idxmin()\n",
    "        balanced_model = quality_data.loc[balanced_idx]\n",
    "        print(f\"  → Use {balanced_model['Model']} (best speed/quality trade-off)\")\n",
    "    \n",
    "    print(\"\\n🎯 For LONG sequences (>4K tokens):\")\n",
    "    print(\"  → Use Performer or LinFormer (proven scalability)\")\n",
    "    print(\"  → Avoid Standard Attention (memory constraints)\")\n",
    "\n",
    "generate_insights(benchmark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Attention Pattern Visualization\n",
    "\n",
    "### Understanding What Linear Attention \"Sees\"\n",
    "\n",
    "Let's visualize and compare the attention patterns produced by different mechanisms to understand how the linear approximations differ from standard attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_patterns():\n",
    "    \"\"\"Visualize and compare attention patterns across different mechanisms.\"\"\"\n",
    "    \n",
    "    # Create synthetic data with clear patterns\n",
    "    seq_len = 64  # Smaller for visualization\n",
    "    d_model = 128\n",
    "    \n",
    "    # Create structured input with different patterns\n",
    "    x = torch.zeros(1, seq_len, d_model)\n",
    "    \n",
    "    # Pattern 1: Periodic signal\n",
    "    for i in range(seq_len):\n",
    "        x[0, i, :d_model//4] = torch.sin(torch.arange(d_model//4, dtype=torch.float) * i * 0.1)\n",
    "    \n",
    "    # Pattern 2: Local dependencies\n",
    "    for i in range(seq_len//2, seq_len):\n",
    "        x[0, i, d_model//4:d_model//2] = x[0, i-1, d_model//4:d_model//2] * 0.9 + 0.1\n",
    "    \n",
    "    # Pattern 3: Long-range dependencies\n",
    "    x[0, seq_len//4:seq_len//2, d_model//2:3*d_model//4] = x[0, 0:seq_len//4, d_model//2:3*d_model//4]\n",
    "    \n",
    "    # Initialize models\n",
    "    models = {\n",
    "        'Standard': StandardAttention(d_model, n_heads=1),  # Single head for clearer visualization\n",
    "        'Performer': PerformerAttention(d_model, n_heads=1, feature_dim=64),\n",
    "        'LinFormer': LinFormerAttention(d_model, n_heads=1, projected_dim=32),\n",
    "    }\n",
    "    \n",
    "    # Get attention patterns\n",
    "    attention_patterns = {}\n",
    "    outputs = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for name, model in models.items():\n",
    "            if name == 'Standard':\n",
    "                output, attn_weights = model(x)\n",
    "                attention_patterns[name] = attn_weights[0, 0].numpy()  # [seq_len, seq_len]\n",
    "            else:\n",
    "                output = model(x)\n",
    "                # For linear attention, we'll compute approximate attention for visualization\n",
    "                attention_patterns[name] = compute_approximate_attention(model, x)\n",
    "            \n",
    "            outputs[name] = output\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle('Attention Pattern Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot attention matrices\n",
    "    for i, (name, pattern) in enumerate(attention_patterns.items()):\n",
    "        ax = axes[0, i]\n",
    "        im = ax.imshow(pattern, cmap='Blues', aspect='auto')\n",
    "        ax.set_title(f'{name} Attention')\n",
    "        ax.set_xlabel('Key Position')\n",
    "        ax.set_ylabel('Query Position')\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Plot attention distributions for specific queries\n",
    "    query_positions = [16, 32, 48]  # Different positions\n",
    "    colors = ['red', 'green', 'blue']\n",
    "    \n",
    "    for i, (name, pattern) in enumerate(attention_patterns.items()):\n",
    "        ax = axes[1, i]\n",
    "        \n",
    "        for j, (pos, color) in enumerate(zip(query_positions, colors)):\n",
    "            ax.plot(pattern[pos], label=f'Query {pos}', color=color, alpha=0.7, linewidth=2)\n",
    "        \n",
    "        ax.set_title(f'{name} Attention Distribution')\n",
    "        ax.set_xlabel('Key Position')\n",
    "        ax.set_ylabel('Attention Weight')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return attention_patterns, outputs\n",
    "\n",
    "\n",
    "def compute_approximate_attention(model, x):\n",
    "    \"\"\"Compute approximate attention weights for linear attention models.\"\"\"\n",
    "    # This is a simplified approximation for visualization\n",
    "    # In practice, linear attention doesn't compute explicit attention matrices\n",
    "    \n",
    "    batch_size, seq_len, d_model = x.shape\n",
    "    \n",
    "    # Get Q, K projections\n",
    "    Q = model.w_q(x).view(batch_size, seq_len, 1, -1).transpose(1, 2)  # Single head\n",
    "    K = model.w_k(x).view(batch_size, seq_len, 1, -1).transpose(1, 2)\n",
    "    \n",
    "    if hasattr(model, 'feature_map'):\n",
    "        # For Performer-like models\n",
    "        Q_feat = model.feature_map(Q)\n",
    "        K_feat = model.feature_map(K)\n",
    "        \n",
    "        # Approximate attention as normalized dot product in feature space\n",
    "        attn_approx = torch.matmul(Q_feat, K_feat.transpose(-2, -1))\n",
    "        attn_approx = F.softmax(attn_approx, dim=-1)\n",
    "        \n",
    "        return attn_approx[0, 0].numpy()\n",
    "    \n",
    "    elif hasattr(model, 'proj_k'):\n",
    "        # For LinFormer-like models\n",
    "        K_proj = model.proj_k(K.transpose(-2, -1)).transpose(-2, -1)\n",
    "        \n",
    "        # Compute attention in projected space\n",
    "        scores = torch.matmul(Q, K_proj.transpose(-2, -1)) / math.sqrt(Q.size(-1))\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Pad to original sequence length for visualization\n",
    "        full_attn = torch.zeros(1, 1, seq_len, seq_len)\n",
    "        proj_dim = K_proj.size(-2)\n",
    "        full_attn[:, :, :, :proj_dim] = attn_weights\n",
    "        \n",
    "        return full_attn[0, 0].numpy()\n",
    "    \n",
    "    else:\n",
    "        # Fallback: compute standard attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(Q.size(-1))\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        return attn_weights[0, 0].numpy()\n",
    "\n",
    "\n",
    "# Run attention pattern visualization\n",
    "attention_patterns, attention_outputs = visualize_attention_patterns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_attention_properties():\n",
    "    \"\"\"Analyze specific properties of different attention mechanisms.\"\"\"\n",
    "    \n",
    "    print(\"🔍 ATTENTION PATTERN ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for name, pattern in attention_patterns.items():\n",
    "        print(f\"\\n📊 {name} Attention Properties:\")\n",
    "        \n",
    "        # 1. Sparsity (how concentrated is attention?)\n",
    "        entropy = -np.sum(pattern * np.log(pattern + 1e-8), axis=-1).mean()\n",
    "        print(f\"  • Average Entropy: {entropy:.3f} (lower = more focused)\")\n",
    "        \n",
    "        # 2. Locality (how much attention to nearby positions?)\n",
    "        seq_len = pattern.shape[0]\n",
    "        local_attention = 0\n",
    "        for i in range(seq_len):\n",
    "            # Attention to positions within ±5 of current position\n",
    "            start = max(0, i - 5)\n",
    "            end = min(seq_len, i + 6)\n",
    "            local_attention += pattern[i, start:end].sum()\n",
    "        local_attention /= seq_len\n",
    "        print(f\"  • Local Attention: {local_attention:.3f} (attention to nearby positions)\")\n",
    "        \n",
    "        # 3. Long-range dependencies\n",
    "        long_range = 0\n",
    "        for i in range(seq_len):\n",
    "            # Attention to positions >10 away\n",
    "            far_positions = np.concatenate([\n",
    "                np.arange(0, max(0, i - 10)),\n",
    "                np.arange(min(seq_len, i + 11), seq_len)\n",
    "            ])\n",
    "            if len(far_positions) > 0:\n",
    "                long_range += pattern[i, far_positions].sum()\n",
    "        long_range /= seq_len\n",
    "        print(f\"  • Long-range Attention: {long_range:.3f} (attention to distant positions)\")\n",
    "        \n",
    "        # 4. Uniformity (how uniform is the attention distribution?)\n",
    "        uniformity = 1.0 / (1.0 + np.var(pattern.mean(axis=0)))\n",
    "        print(f\"  • Uniformity: {uniformity:.3f} (higher = more uniform)\")\n",
    "        \n",
    "        # 5. Diagonal dominance (attention to same position)\n",
    "        diagonal_strength = np.diag(pattern).mean()\n",
    "        print(f\"  • Self-attention: {diagonal_strength:.3f} (attention to same position)\")\n",
    "    \n",
    "    # Compare outputs\n",
    "    print(\"\\n🎯 OUTPUT SIMILARITY ANALYSIS:\")\n",
    "    reference_output = attention_outputs['Standard']\n",
    "    \n",
    "    for name, output in attention_outputs.items():\n",
    "        if name != 'Standard':\n",
    "            # Cosine similarity\n",
    "            cos_sim = F.cosine_similarity(\n",
    "                reference_output.flatten(), output.flatten(), dim=0\n",
    "            ).item()\n",
    "            \n",
    "            # MSE\n",
    "            mse = F.mse_loss(reference_output, output).item()\n",
    "            \n",
    "            # Correlation coefficient\n",
    "            ref_flat = reference_output.flatten().numpy()\n",
    "            out_flat = output.flatten().numpy()\n",
    "            correlation = np.corrcoef(ref_flat, out_flat)[0, 1]\n",
    "            \n",
    "            print(f\"\\n{name} vs Standard:\")\n",
    "            print(f\"  • Cosine Similarity: {cos_sim:.4f}\")\n",
    "            print(f\"  • MSE: {mse:.6f}\")\n",
    "            print(f\"  • Correlation: {correlation:.4f}\")\n",
    "\n",
    "analyze_attention_properties()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Practical Implementation Guidelines\n",
    "\n",
    "### When to Use Each Mechanism\n",
    "\n",
    "Based on our analysis, here are practical guidelines for choosing linear attention mechanisms:\n",
    "\n",
    "#### 🎯 **Decision Framework**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_recommendation_system():\n",
    "    \"\"\"Interactive system to recommend attention mechanisms based on requirements.\"\"\"\n",
    "    \n",
    "    print(\"🤖 ATTENTION MECHANISM RECOMMENDATION SYSTEM\")\n",
    "    print(\"=\" * 55)\n",
    "    print(\"Answer a few questions to get personalized recommendations!\\n\")\n",
    "    \n",
    "    # Define decision tree\n",
    "    recommendations = {\n",
    "        'sequence_length': {\n",
    "            'short': '<= 512 tokens',\n",
    "            'medium': '513-2048 tokens', \n",
    "            'long': '2049-8192 tokens',\n",
    "            'very_long': '> 8192 tokens'\n",
    "        },\n",
    "        'priority': {\n",
    "            'speed': 'Maximum computational speed',\n",
    "            'memory': 'Minimal memory usage',\n",
    "            'quality': 'Best approximation quality',\n",
    "            'balanced': 'Balance of speed/memory/quality'\n",
    "        },\n",
    "        'task_type': {\n",
    "            'classification': 'Text classification, sentiment analysis',\n",
    "            'generation': 'Text generation, language modeling',\n",
    "            'understanding': 'Reading comprehension, QA',\n",
    "            'translation': 'Machine translation, seq2seq'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Recommendation logic\n",
    "    def get_recommendation(seq_len, priority, task):\n",
    "        recommendations = []\n",
    "        \n",
    "        if seq_len == 'short':\n",
    "            recommendations.append((\"Standard Attention\", \"Manageable complexity, best quality\", \"⭐⭐⭐\"))\n",
    "            if priority in ['speed', 'memory']:\n",
    "                recommendations.append((\"Performer\", \"Good approximation with speed benefits\", \"⭐⭐\"))\n",
    "        \n",
    "        elif seq_len == 'medium':\n",
    "            if priority == 'quality':\n",
    "                recommendations.append((\"Performer (Orthogonal)\", \"Best linear approximation\", \"⭐⭐⭐\"))\n",
    "            elif priority == 'speed':\n",
    "                recommendations.append((\"LinFormer\", \"Excellent speed-quality balance\", \"⭐⭐⭐\"))\n",
    "            elif priority == 'memory':\n",
    "                recommendations.append((\"Linear Transformer\", \"Memory efficient for generation\", \"⭐⭐⭐\"))\n",
    "            else:  # balanced\n",
    "                recommendations.append((\"Performer\", \"Good all-around performance\", \"⭐⭐⭐\"))\n",
    "        \n",
    "        elif seq_len == 'long':\n",
    "            recommendations.append((\"LinFormer\", \"Proven scalability, good quality\", \"⭐⭐⭐\"))\n",
    "            recommendations.append((\"Performer\", \"Fast approximation\", \"⭐⭐\"))\n",
    "            if task == 'generation':\n",
    "                recommendations.append((\"Linear Transformer\", \"Causal attention for generation\", \"⭐⭐\"))\n",
    "        \n",
    "        else:  # very_long\n",
    "            recommendations.append((\"LinFormer\", \"Only viable option for very long sequences\", \"⭐⭐⭐\"))\n",
    "            if priority == 'speed':\n",
    "                recommendations.append((\"FNet\", \"Ultra-fast for specific tasks\", \"⭐⭐\"))\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    # Example scenarios\n",
    "    scenarios = [\n",
    "        ('medium', 'balanced', 'classification', \"Document Classification (1024 tokens)\"),\n",
    "        ('long', 'speed', 'generation', \"Long-form Text Generation (4096 tokens)\"),\n",
    "        ('short', 'quality', 'understanding', \"Reading Comprehension (256 tokens)\"),\n",
    "        ('very_long', 'memory', 'translation', \"Long Document Translation (16K tokens)\")\n",
    "    ]\n",
    "    \n",
    "    for seq_len, priority, task, description in scenarios:\n",
    "        print(f\"📋 Scenario: {description}\")\n",
    "        print(f\"   Sequence Length: {recommendations['sequence_length'][seq_len]}\")\n",
    "        print(f\"   Priority: {recommendations['priority'][priority]}\")\n",
    "        print(f\"   Task: {recommendations['task_type'][task]}\")\n",
    "        print(\"\\n   🎯 Recommendations:\")\n",
    "        \n",
    "        recs = get_recommendation(seq_len, priority, task)\n",
    "        for i, (method, reason, rating) in enumerate(recs, 1):\n",
    "            print(f\"      {i}. {method} {rating}\")\n",
    "            print(f\"         → {reason}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "attention_recommendation_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Implementation Tips and Best Practices\n",
    "\n",
    "### Training Considerations\n",
    "\n",
    "Here are key implementation tips for successful deployment of linear attention mechanisms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final demonstration: Simple usage example\n",
    "def simple_usage_example():\n",
    "    \"\"\"Demonstrate simple usage of KV caching.\"\"\"\n",
    "    \n",
    "    print(\"=== Simple Usage Example ===\")\n",
    "    print(\"This example shows how to use KV caching in your own projects.\\n\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = CachedAttention(d_model=256, n_heads=8)\n",
    "    \n",
    "    # Generate text step by step\n",
    "    batch_size, d_model = 1, 256\n",
    "    kv_cache = None\n",
    "    \n",
    "    print(\"Generating sequence step by step:\")\n",
    "    \n",
    "    for step in range(5):\n",
    "        # Simulate new token\n",
    "        new_token = torch.randn(batch_size, 1, d_model)\n",
    "        \n",
    "        # Process with caching\n",
    "        output, kv_cache = model(\n",
    "            new_token, \n",
    "            kv_cache=kv_cache, \n",
    "            use_cache=True\n",
    "        )\n",
    "        \n",
    "        cache_info = kv_cache.get_cache_info() if kv_cache else \"No cache\"\n",
    "        print(f\"Step {step + 1}: Processed token, {cache_info}\")\n",
    "    \n",
    "    print(\"\\n✅ Successfully demonstrated KV caching!\")\n",
    "    print(\"\\n🎯 Next steps:\")\n",
    "    print(\"   • Integrate into your transformer model\")\n",
    "    print(\"   • Benchmark on your specific use case\")\n",
    "    print(\"   • Consider advanced optimizations based on your requirements\")\n",
    "\n",
    "simple_usage_example()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
