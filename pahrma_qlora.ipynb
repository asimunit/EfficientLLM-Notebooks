{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "832066a5",
   "metadata": {},
   "source": [
    "# Pharmaceutical Multi-Label Classification with QLoRA Fine-tuning\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "**Objective**: Improve pharmaceutical document classification from 80% to 95%+ F1 score using QLoRA fine-tuned Llama 3.1 8B\n",
    "\n",
    "**Business Context**: \n",
    "- Current rule-based + traditional ML system achieves 80% F1 score\n",
    "- Need to classify pharmaceutical documents into multiple therapeutic areas, regulatory categories, and risk levels\n",
    "- Critical for drug development, regulatory compliance, and pharmacovigilance\n",
    "\n",
    "**Key Challenges**:\n",
    "- Complex medical terminology and drug interactions\n",
    "- Multiple overlapping categories per document\n",
    "- Regulatory compliance requirements (FDA, EMA)\n",
    "- Class imbalance in rare disease categories\n",
    "- Need for explainable predictions\n",
    "\n",
    "## Table of Contents\n",
    "1. Environment Setup and Dependencies\n",
    "2. Pharmaceutical Data Preparation\n",
    "3. Multi-Label Classification Framework\n",
    "4. QLoRA Configuration for Classification\n",
    "5. Model Architecture and Training\n",
    "6. Evaluation Metrics and Benchmarking\n",
    "7. Ablation Studies and Optimization\n",
    "8. Production Deployment and Monitoring\n",
    "9. Regulatory Compliance and Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfb72cf",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4629e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for pharmaceutical NLP and QLoRA\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers>=4.31.0\n",
    "!pip install peft>=0.4.0\n",
    "!pip install datasets\n",
    "!pip install bitsandbytes>=0.39.0\n",
    "!pip install accelerate>=0.20.3\n",
    "!pip install trl\n",
    "!pip install scikit-learn\n",
    "!pip install scipy\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install wandb\n",
    "!pip install nltk\n",
    "!pip install spacy\n",
    "!pip install scispacy  # For biomedical NLP\n",
    "!pip install regex\n",
    "!pip install wordcloud\n",
    "!pip install plotly\n",
    "\n",
    "# Download medical NLP models\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_sci_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7bc394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score, \n",
    "    classification_report, multilabel_confusion_matrix,\n",
    "    hamming_loss, jaccard_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4659afdc",
   "metadata": {},
   "source": [
    "## 2. Pharmaceutical Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6ecfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define pharmaceutical classification taxonomy\n",
    "PHARMA_LABELS = {\n",
    "    # Therapeutic Areas (Primary Classification)\n",
    "    'therapeutic_areas': [\n",
    "        'oncology', 'cardiovascular', 'neurology', 'immunology', 'infectious_diseases',\n",
    "        'respiratory', 'endocrinology', 'gastroenterology', 'dermatology', 'ophthalmology',\n",
    "        'psychiatry', 'rheumatology', 'urology', 'hematology', 'rare_diseases'\n",
    "    ],\n",
    "    \n",
    "    # Drug Development Phase\n",
    "    'development_phase': [\n",
    "        'preclinical', 'phase_1', 'phase_2', 'phase_3', 'phase_4', 'post_market'\n",
    "    ],\n",
    "    \n",
    "    # Regulatory Categories\n",
    "    'regulatory': [\n",
    "        'safety_report', 'efficacy_study', 'pharmacokinetics', 'pharmacodynamics',\n",
    "        'toxicology', 'clinical_trial', 'regulatory_submission', 'label_update'\n",
    "    ],\n",
    "    \n",
    "    # Risk Assessment\n",
    "    'risk_level': [\n",
    "        'low_risk', 'medium_risk', 'high_risk', 'critical_risk'\n",
    "    ],\n",
    "    \n",
    "    # Document Type\n",
    "    'document_type': [\n",
    "        'clinical_study_report', 'adverse_event_report', 'drug_label', \n",
    "        'investigator_brochure', 'protocol', 'statistical_analysis_plan',\n",
    "        'pharmacovigilance_report', 'regulatory_correspondence'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Flatten all labels\n",
    "ALL_LABELS = []\n",
    "for category, labels in PHARMA_LABELS.items():\n",
    "    ALL_LABELS.extend(labels)\n",
    "\n",
    "print(f\"Total classification labels: {len(ALL_LABELS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88902fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED: Generate realistic pharmaceutical training data\n",
    "def generate_pharma_training_data(n_samples: int = 2000) -> List[Dict]:\n",
    "    \"\"\"Generate realistic pharmaceutical documents with multi-label classifications.\"\"\"\n",
    "    \n",
    "    # Document templates by type - FIXED formatting issues\n",
    "    document_templates = {\n",
    "        'clinical_study_report': [\n",
    "            \"Phase {} clinical trial evaluating {} in patients with {}. Primary endpoint: {}. Secondary endpoints include safety, tolerability, and pharmacokinetics. {} patients enrolled across {} sites. Results demonstrate {} with {} adverse events reported.\",\n",
    "            \"Randomized controlled trial of {} vs placebo in {} patients. Study duration: {} months. Primary efficacy endpoint met with statistical significance (p<0.05). Safety profile consistent with known {} risk profile.\",\n",
    "            \"Open-label extension study of {} in {} indication. Long-term safety and efficacy data collected over {} years. No new safety signals identified. Sustained efficacy observed in {}% of patients.\"\n",
    "        ],\n",
    "        'adverse_event_report': [\n",
    "            \"Serious adverse event report: {} year old {} patient experienced {} after {} days of {} treatment. Event assessed as {} related to study drug. Patient {} and treatment {}.\",\n",
    "            \"Spontaneous report of {} in patient taking {} for {}. Onset {} hours post-dose. Concomitant medications: {}. Dechallenge: {}. Rechallenge: {}.\",\n",
    "            \"Healthcare professional report: {} observed in {} patients receiving {}. All cases {} and {} within {} days. Causality assessment: {}.\"\n",
    "        ],\n",
    "        'drug_label': [\n",
    "            \"{} is indicated for the treatment of {} in adult patients. Recommended dose: {}. Contraindications: {}. Warnings and precautions: monitor for {}.\",\n",
    "            \"INDICATIONS AND USAGE: {} is a {} indicated for {} treatment. DOSAGE AND ADMINISTRATION: {}. CONTRAINDICATIONS: {}. WARNINGS: {}.\",\n",
    "            \"Prescribing information for {}: {} tablets. Indicated for {} treatment. Common adverse reactions (â‰¥5%): {}. Drug interactions: {}.\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Medical terminology pools\n",
    "    drug_names = [\n",
    "        'Pembrolizumab', 'Adalimumab', 'Rituximab', 'Bevacizumab', 'Trastuzumab',\n",
    "        'Infliximab', 'Etanercept', 'Tocilizumab', 'Nivolumab', 'Atezolizumab',\n",
    "        'Durvalumab', 'Avelumab', 'Ipilimumab', 'Cetuximab', 'Panitumumab'\n",
    "    ]\n",
    "    \n",
    "    drug_classes = [\n",
    "        'monoclonal antibody', 'TNF-alpha inhibitor', 'PD-1 inhibitor', \n",
    "        'VEGF inhibitor', 'HER2 inhibitor', 'CD20 antagonist', 'IL-6 inhibitor'\n",
    "    ]\n",
    "    \n",
    "    conditions = [\n",
    "        'non-small cell lung cancer', 'breast cancer', 'colorectal cancer', 'melanoma',\n",
    "        'rheumatoid arthritis', 'Crohn\\'s disease', 'multiple sclerosis', 'psoriasis',\n",
    "        'atrial fibrillation', 'heart failure', 'diabetes mellitus', 'hypertension',\n",
    "        'COPD', 'asthma', 'depression', 'schizophrenia', 'Alzheimer\\'s disease'\n",
    "    ]\n",
    "    \n",
    "    adverse_events = [\n",
    "        'neutropenia', 'thrombocytopenia', 'hepatotoxicity', 'cardiotoxicity',\n",
    "        'pneumonitis', 'colitis', 'dermatitis', 'infusion reaction',\n",
    "        'nausea', 'fatigue', 'diarrhea', 'headache', 'hypertension'\n",
    "    ]\n",
    "    \n",
    "    training_data = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Select document type and template\n",
    "        doc_type = random.choice(list(document_templates.keys()))\n",
    "        template = random.choice(document_templates[doc_type])\n",
    "        \n",
    "        # Generate document content based on type - FIXED argument count\n",
    "        if doc_type == 'clinical_study_report':\n",
    "            if 'Phase {}' in template:\n",
    "                content = template.format(\n",
    "                    random.choice(['I', 'II', 'III']),\n",
    "                    random.choice(drug_names),\n",
    "                    random.choice(conditions),\n",
    "                    random.choice(['overall survival', 'progression-free survival', 'response rate']),\n",
    "                    random.randint(50, 500),\n",
    "                    random.randint(5, 50),\n",
    "                    random.choice(['significant improvement', 'positive results', 'favorable outcomes']),\n",
    "                    random.choice(['minimal', 'manageable', 'expected'])\n",
    "                )\n",
    "            elif 'Randomized controlled' in template:\n",
    "                content = template.format(\n",
    "                    random.choice(drug_names),\n",
    "                    random.randint(100, 1000),\n",
    "                    random.randint(6, 36),\n",
    "                    random.choice(['acceptable', 'manageable', 'expected'])\n",
    "                )\n",
    "            else:  # Open-label extension\n",
    "                content = template.format(\n",
    "                    random.choice(drug_names),\n",
    "                    random.choice(conditions),\n",
    "                    random.randint(2, 5),\n",
    "                    random.randint(60, 85)\n",
    "                )\n",
    "                \n",
    "        elif doc_type == 'adverse_event_report':\n",
    "            if 'Serious adverse event' in template:\n",
    "                content = template.format(\n",
    "                    random.randint(18, 85),\n",
    "                    random.choice(['male', 'female']),\n",
    "                    random.choice(adverse_events),\n",
    "                    random.randint(1, 30),\n",
    "                    random.choice(drug_names),\n",
    "                    random.choice(['possibly', 'probably', 'definitely']),\n",
    "                    random.choice(['recovered', 'recovering', 'not recovered']),\n",
    "                    random.choice(['discontinued', 'continued', 'dose reduced'])\n",
    "                )\n",
    "            elif 'Spontaneous report' in template:\n",
    "                content = template.format(\n",
    "                    random.choice(adverse_events),\n",
    "                    random.choice(drug_names),\n",
    "                    random.choice(conditions),\n",
    "                    random.randint(1, 72),\n",
    "                    random.choice(['aspirin', 'metformin', 'none']),\n",
    "                    random.choice(['positive', 'negative', 'not done']),\n",
    "                    random.choice(['positive', 'negative', 'not done'])\n",
    "                )\n",
    "            else:  # Healthcare professional report\n",
    "                content = template.format(\n",
    "                    random.choice(adverse_events),\n",
    "                    random.randint(2, 10),\n",
    "                    random.choice(drug_names),\n",
    "                    random.choice(['resolved', 'ongoing', 'fatal']),\n",
    "                    random.choice(['recovered', 'improving', 'worsened']),\n",
    "                    random.randint(1, 14),\n",
    "                    random.choice(['probable', 'possible', 'unlikely'])\n",
    "                )\n",
    "                \n",
    "        else:  # drug_label - FIXED with proper argument count\n",
    "            if 'INDICATIONS AND USAGE' in template:\n",
    "                content = template.format(\n",
    "                    random.choice(drug_names),\n",
    "                    random.choice(drug_classes),\n",
    "                    random.choice(conditions),\n",
    "                    f\"{random.randint(5, 100)} mg {random.choice(['daily', 'twice daily', 'weekly'])}\",\n",
    "                    random.choice(['pregnancy', 'severe hepatic impairment', 'hypersensitivity']),\n",
    "                    random.choice(['hepatic function', 'renal function', 'cardiac function'])\n",
    "                )\n",
    "            elif 'Prescribing information' in template:\n",
    "                content = template.format(\n",
    "                    random.choice(drug_names),\n",
    "                    f\"{random.randint(5, 100)} mg\",\n",
    "                    random.choice(conditions),\n",
    "                    random.choice(['nausea, headache, fatigue', 'diarrhea, rash, dizziness']),\n",
    "                    random.choice(['warfarin, digoxin', 'CYP3A4 inhibitors', 'none known'])\n",
    "                )\n",
    "            else:  # Simple format\n",
    "                content = template.format(\n",
    "                    random.choice(drug_names),\n",
    "                    random.choice(conditions),\n",
    "                    f\"{random.randint(5, 100)} mg {random.choice(['daily', 'twice daily', 'weekly'])}\",\n",
    "                    random.choice(['pregnancy', 'severe hepatic impairment', 'hypersensitivity']),\n",
    "                    random.choice(['hepatic function', 'renal function', 'cardiac function'])\n",
    "                )\n",
    "        \n",
    "        # Assign labels based on content and document type\n",
    "        labels = [doc_type]\n",
    "        \n",
    "        # Add therapeutic area labels\n",
    "        if any(cancer in content.lower() for cancer in ['cancer', 'tumor', 'oncology', 'melanoma']):\n",
    "            labels.append('oncology')\n",
    "        if any(cardio in content.lower() for cardio in ['heart', 'cardiac', 'atrial', 'cardiovascular']):\n",
    "            labels.append('cardiovascular')\n",
    "        if any(neuro in content.lower() for neuro in ['multiple sclerosis', 'alzheimer', 'depression', 'schizophrenia']):\n",
    "            labels.append('neurology')\n",
    "        if any(immune in content.lower() for immune in ['rheumatoid', 'crohn', 'psoriasis', 'adalimumab', 'infliximab']):\n",
    "            labels.append('immunology')\n",
    "        \n",
    "        # Add development phase\n",
    "        if 'phase i' in content.lower() or 'phase 1' in content:\n",
    "            labels.append('phase_1')\n",
    "        elif 'phase ii' in content.lower() or 'phase 2' in content:\n",
    "            labels.append('phase_2')\n",
    "        elif 'phase iii' in content.lower() or 'phase 3' in content:\n",
    "            labels.append('phase_3')\n",
    "        elif 'post-market' in content.lower() or 'post market' in content.lower():\n",
    "            labels.append('post_market')\n",
    "        \n",
    "        # Add regulatory categories\n",
    "        if doc_type == 'adverse_event_report':\n",
    "            labels.extend(['safety_report', 'pharmacovigilance_report'])\n",
    "        elif doc_type == 'clinical_study_report':\n",
    "            labels.extend(['efficacy_study', 'clinical_trial'])\n",
    "        \n",
    "        # Add risk level\n",
    "        if any(serious in content.lower() for serious in ['serious', 'severe', 'critical', 'death']):\n",
    "            labels.append('high_risk')\n",
    "        elif any(moderate in content.lower() for moderate in ['moderate', 'significant']):\n",
    "            labels.append('medium_risk')\n",
    "        else:\n",
    "            labels.append('low_risk')\n",
    "        \n",
    "        # Remove duplicates and ensure valid labels\n",
    "        labels = list(set([label for label in labels if label in ALL_LABELS]))\n",
    "        \n",
    "        training_data.append({\n",
    "            'text': content,\n",
    "            'labels': labels,\n",
    "            'doc_type': doc_type\n",
    "        })\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "# Generate training data\n",
    "print(\"Generating pharmaceutical training data...\")\n",
    "pharma_data = generate_pharma_training_data(1000)  # Reduced for demonstration\n",
    "\n",
    "print(f\"Generated {len(pharma_data)} pharmaceutical documents\")\n",
    "print(f\"\\nSample document:\")\n",
    "sample = pharma_data[0]\n",
    "print(f\"Text: {sample['text'][:200]}...\")\n",
    "print(f\"Labels: {sample['labels']}\")\n",
    "print(f\"Document type: {sample['doc_type']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236a72e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze label distribution and dataset statistics\n",
    "def analyze_dataset_statistics(data: List[Dict]):\n",
    "    \"\"\"Analyze the generated dataset for balance and coverage.\"\"\"\n",
    "    \n",
    "    print(\"Dataset Statistics Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic statistics\n",
    "    text_lengths = [len(doc['text'].split()) for doc in data]\n",
    "    label_counts = [len(doc['labels']) for doc in data]\n",
    "    \n",
    "    print(f\"Total documents: {len(data)}\")\n",
    "    print(f\"Average text length: {np.mean(text_lengths):.1f} words\")\n",
    "    print(f\"Text length range: {min(text_lengths)} - {max(text_lengths)} words\")\n",
    "    print(f\"Average labels per document: {np.mean(label_counts):.1f}\")\n",
    "    print(f\"Labels per document range: {min(label_counts)} - {max(label_counts)}\")\n",
    "    \n",
    "    # Label frequency analysis\n",
    "    all_labels = []\n",
    "    for doc in data:\n",
    "        all_labels.extend(doc['labels'])\n",
    "    \n",
    "    label_freq = Counter(all_labels)\n",
    "    \n",
    "    print(f\"\\nLabel Frequency Analysis:\")\n",
    "    print(f\"Total unique labels used: {len(label_freq)}\")\n",
    "    print(f\"Most common labels:\")\n",
    "    for label, count in label_freq.most_common(10):\n",
    "        print(f\"  {label}: {count} ({count/len(data)*100:.1f}%)\")\n",
    "    \n",
    "    # Check for class imbalance\n",
    "    min_freq = min(label_freq.values())\n",
    "    max_freq = max(label_freq.values())\n",
    "    imbalance_ratio = max_freq / min_freq\n",
    "    \n",
    "    print(f\"\\nClass Imbalance Analysis:\")\n",
    "    print(f\"Most frequent label: {max_freq} documents\")\n",
    "    print(f\"Least frequent label: {min_freq} documents\")\n",
    "    print(f\"Imbalance ratio: {imbalance_ratio:.1f}:1\")\n",
    "    \n",
    "    if imbalance_ratio > 10:\n",
    "        print(\"âš ï¸  Significant class imbalance detected - consider balancing strategies\")\n",
    "    \n",
    "    # Document type distribution\n",
    "    doc_types = Counter([doc['doc_type'] for doc in data])\n",
    "    print(f\"\\nDocument Type Distribution:\")\n",
    "    for doc_type, count in doc_types.items():\n",
    "        print(f\"  {doc_type}: {count} ({count/len(data)*100:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'label_freq': label_freq,\n",
    "        'text_lengths': text_lengths,\n",
    "        'label_counts': label_counts,\n",
    "        'imbalance_ratio': imbalance_ratio\n",
    "    }\n",
    "\n",
    "# Analyze the dataset\n",
    "stats = analyze_dataset_statistics(pharma_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64d08e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset characteristics\n",
    "def visualize_dataset_statistics(data: List[Dict], stats: Dict):\n",
    "    \"\"\"Create visualizations for dataset analysis.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Text length distribution\n",
    "    axes[0, 0].hist(stats['text_lengths'], bins=30, alpha=0.7, color='skyblue')\n",
    "    axes[0, 0].set_title('Distribution of Document Lengths')\n",
    "    axes[0, 0].set_xlabel('Number of Words')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].axvline(np.mean(stats['text_lengths']), color='red', linestyle='--', label='Mean')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Labels per document distribution\n",
    "    axes[0, 1].hist(stats['label_counts'], bins=range(1, max(stats['label_counts'])+2), \n",
    "                    alpha=0.7, color='lightgreen')\n",
    "    axes[0, 1].set_title('Distribution of Labels per Document')\n",
    "    axes[0, 1].set_xlabel('Number of Labels')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].axvline(np.mean(stats['label_counts']), color='red', linestyle='--', label='Mean')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Top 15 most frequent labels\n",
    "    top_labels = dict(stats['label_freq'].most_common(15))\n",
    "    axes[1, 0].barh(range(len(top_labels)), list(top_labels.values()), color='orange', alpha=0.7)\n",
    "    axes[1, 0].set_yticks(range(len(top_labels)))\n",
    "    axes[1, 0].set_yticklabels(list(top_labels.keys()))\n",
    "    axes[1, 0].set_title('Top 15 Most Frequent Labels')\n",
    "    axes[1, 0].set_xlabel('Frequency')\n",
    "    \n",
    "    # Document type distribution\n",
    "    doc_types = Counter([doc['doc_type'] for doc in data])\n",
    "    axes[1, 1].pie(doc_types.values(), labels=doc_types.keys(), autopct='%1.1f%%', \n",
    "                   colors=['lightcoral', 'lightblue', 'lightgreen'])\n",
    "    axes[1, 1].set_title('Document Type Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Label co-occurrence heatmap\n",
    "    print(\"\\nGenerating label co-occurrence analysis...\")\n",
    "    \n",
    "    # Create binary matrix for labels\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    label_matrix = mlb.fit_transform([doc['labels'] for doc in data])\n",
    "    label_names = mlb.classes_\n",
    "    \n",
    "    # Calculate co-occurrence matrix\n",
    "    cooccurrence = np.dot(label_matrix.T, label_matrix)\n",
    "    \n",
    "    # Normalize by diagonal (convert to conditional probability)\n",
    "    diag = np.diag(cooccurrence)\n",
    "    cooccurrence_norm = cooccurrence / diag[:, np.newaxis]\n",
    "    \n",
    "    # Plot heatmap for most frequent labels\n",
    "    top_15_indices = [i for i, label in enumerate(label_names) \n",
    "                      if label in dict(stats['label_freq'].most_common(15))]\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cooccurrence_norm[np.ix_(top_15_indices, top_15_indices)], \n",
    "                xticklabels=[label_names[i] for i in top_15_indices],\n",
    "                yticklabels=[label_names[i] for i in top_15_indices],\n",
    "                annot=True, fmt='.2f', cmap='Blues')\n",
    "    plt.title('Label Co-occurrence Matrix (Top 15 Labels)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create visualizations\n",
    "visualize_dataset_statistics(pharma_data, stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f77d53-4faf-44b3-aefd-1721add449d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pharma_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6873a0b3",
   "metadata": {},
   "source": [
    "## 3. Multi-Label Classification Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a3b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for multi-label classification training\n",
    "def prepare_classification_data(data: List[Dict], test_size: float = 0.2, val_size: float = 0.1):\n",
    "    \"\"\"Prepare data for multi-label classification with proper splits.\"\"\"\n",
    "    \n",
    "    # Convert to format suitable for training\n",
    "    texts = [doc['text'] for doc in data]\n",
    "    labels = [doc['labels'] for doc in data]\n",
    "    \n",
    "    # Create stratified split to maintain label distribution\n",
    "    # For multi-label, we'll use iterative stratification approach\n",
    "    train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "        texts, labels, test_size=(test_size + val_size), random_state=42\n",
    "    )\n",
    "    \n",
    "    # Split temp into validation and test\n",
    "    val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "        temp_texts, temp_labels, \n",
    "        test_size=test_size/(test_size + val_size), \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Data split:\")\n",
    "    print(f\"  Training: {len(train_texts)} documents\")\n",
    "    print(f\"  Validation: {len(val_texts)} documents\")\n",
    "    print(f\"  Test: {len(test_texts)} documents\")\n",
    "    \n",
    "    # Create label encoder\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    mlb.fit(train_labels + val_labels + test_labels)\n",
    "    \n",
    "    print(f\"\\nTotal unique labels: {len(mlb.classes_)}\")\n",
    "    print(f\"Label classes: {list(mlb.classes_)}\")\n",
    "    \n",
    "    return {\n",
    "        'train': {'texts': train_texts, 'labels': train_labels},\n",
    "        'val': {'texts': val_texts, 'labels': val_labels},\n",
    "        'test': {'texts': test_texts, 'labels': test_labels},\n",
    "        'label_encoder': mlb\n",
    "    }\n",
    "\n",
    "# Prepare the data\n",
    "data_splits = prepare_classification_data(pharma_data)\n",
    "mlb = data_splits['label_encoder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5798a6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instruction-based training format for classification\n",
    "def create_classification_instruction(text: str, labels: List[str], is_training: bool = True) -> str:\n",
    "    \"\"\"Create instruction-following format for multi-label classification.\"\"\"\n",
    "    \n",
    "    instruction = \"\"\"\n",
    "You are an expert pharmaceutical document classifier. Your task is to analyze pharmaceutical documents and assign appropriate labels from the following categories:\n",
    "\n",
    "THERAPEUTIC AREAS: oncology, cardiovascular, neurology, immunology, infectious_diseases, respiratory, endocrinology, gastroenterology, dermatology, ophthalmology, psychiatry, rheumatology, urology, hematology, rare_diseases\n",
    "\n",
    "DEVELOPMENT PHASE: preclinical, phase_1, phase_2, phase_3, phase_4, post_market\n",
    "\n",
    "REGULATORY CATEGORIES: safety_report, efficacy_study, pharmacokinetics, pharmacodynamics, toxicology, clinical_trial, regulatory_submission, label_update\n",
    "\n",
    "RISK LEVEL: low_risk, medium_risk, high_risk, critical_risk\n",
    "\n",
    "DOCUMENT TYPE: clinical_study_report, adverse_event_report, drug_label, investigator_brochure, protocol, statistical_analysis_plan, pharmacovigilance_report, regulatory_correspondence\n",
    "\n",
    "Analyze the following pharmaceutical document and provide a comma-separated list of applicable labels:\n",
    "\"\"\"\n",
    "    \n",
    "    if is_training:\n",
    "        # Training format with expected output\n",
    "        formatted_text = f\"\"\"{instruction.strip()}\n",
    "\n",
    "DOCUMENT:\n",
    "{text}\n",
    "\n",
    "LABELS: {', '.join(sorted(labels))}\"\"\"\n",
    "    else:\n",
    "        # Inference format without labels\n",
    "        formatted_text = f\"\"\"{instruction.strip()}\n",
    "\n",
    "DOCUMENT:\n",
    "{text}\n",
    "\n",
    "LABELS:\"\"\"\n",
    "    \n",
    "    return formatted_text\n",
    "\n",
    "# Convert data to instruction format\n",
    "def create_instruction_dataset(data_dict: Dict) -> DatasetDict:\n",
    "    \"\"\"Convert data to instruction-following format.\"\"\"\n",
    "    \n",
    "    datasets = {}\n",
    "    \n",
    "    for split_name, split_data in data_dict.items():\n",
    "        if split_name == 'label_encoder':\n",
    "            continue\n",
    "            \n",
    "        formatted_examples = []\n",
    "        for text, labels in zip(split_data['texts'], split_data['labels']):\n",
    "            formatted_text = create_classification_instruction(text, labels, is_training=True)\n",
    "            formatted_examples.append({'text': formatted_text})\n",
    "        \n",
    "        datasets[split_name] = Dataset.from_list(formatted_examples)\n",
    "    \n",
    "    return DatasetDict(datasets)\n",
    "\n",
    "# Create instruction datasets\n",
    "instruction_datasets = create_instruction_dataset(data_splits)\n",
    "\n",
    "print(\"Created instruction-following datasets:\")\n",
    "for split_name, dataset in instruction_datasets.items():\n",
    "    print(f\"  {split_name}: {len(dataset)} examples\")\n",
    "\n",
    "print(\"\\nSample instruction format:\")\n",
    "print(\"=\" * 80)\n",
    "sample_text = instruction_datasets['train'][0]['text']\n",
    "print(sample_text[:800] + \"...\" if len(sample_text) > 800 else sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a9b9fc",
   "metadata": {},
   "source": [
    "## 4. QLoRA Configuration for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d74fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized QLoRA configuration for classification tasks\n",
    "def create_classification_bnb_config():\n",
    "    \"\"\"Create BitsAndBytes configuration optimized for classification.\"\"\"\n",
    "    \n",
    "    return BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "def create_classification_lora_config():\n",
    "    \"\"\"Create LoRA configuration optimized for classification.\"\"\"\n",
    "    \n",
    "    return LoraConfig(\n",
    "        r=128,  # Higher rank for better classification performance\n",
    "        lora_alpha=32,  # Balanced scaling\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        ],\n",
    "        lora_dropout=0.05,  # Lower dropout for classification\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "\n",
    "# Model configuration\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "new_model = \"llama-3.1-8b-pharma-classifier-qlora\"\n",
    "\n",
    "# Create configurations\n",
    "quantization_config = create_classification_bnb_config()\n",
    "lora_config = create_classification_lora_config()\n",
    "\n",
    "print(\"Classification QLoRA Configuration:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"LoRA rank (r): {lora_config.r}\")\n",
    "print(f\"LoRA alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"LoRA dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"Target modules: {len(lora_config.target_modules)}\")\n",
    "print(f\"Quantization: 4-bit NF4 with double quantization\")\n",
    "print(f\"Expected trainable parameters: ~{lora_config.r * len(lora_config.target_modules) * 2 * 4096:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e321b847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory monitoring utilities\n",
    "def print_memory_usage(stage=\"\"):\n",
    "    \"\"\"Print current memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        gpu_allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        gpu_reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "        \n",
    "        print(f\"\\n{stage} GPU Memory:\")\n",
    "        print(f\"  Allocated: {gpu_allocated:.1f} GB / {gpu_memory:.1f} GB ({gpu_allocated/gpu_memory*100:.1f}%)\")\n",
    "        print(f\"  Reserved: {gpu_reserved:.1f} GB ({gpu_reserved/gpu_memory*100:.1f}%)\")\n",
    "\n",
    "print_memory_usage(\"Initial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03f4ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print_memory_usage(\"After tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdae3af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with quantization\n",
    "print(\"Loading Llama 3.1 8B with 4-bit quantization...\")\n",
    "print(\"This will take a few minutes...\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",  # Use flash attention for efficiency\n",
    ")\n",
    "\n",
    "print_memory_usage(\"After model loading\")\n",
    "print(\"âœ… Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c620cd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "print_memory_usage(\"After LoRA setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d47685",
   "metadata": {},
   "source": [
    "## 5. Model Architecture and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293853e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced training configuration for classification\n",
    "def create_classification_training_args(output_dir: str, num_epochs: int = 5):\n",
    "    \"\"\"Create optimized training arguments for classification.\"\"\"\n",
    "    \n",
    "    return TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=16,  # Effective batch size = 16\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        save_steps=100,\n",
    "        logging_steps=25,\n",
    "        learning_rate=1e-4,  # Conservative for classification\n",
    "        weight_decay=0.01,\n",
    "        fp16=False,\n",
    "        bf16=True,\n",
    "        max_grad_norm=0.3,\n",
    "        max_steps=-1,\n",
    "        warmup_ratio=0.1,  # More warmup for stability\n",
    "        group_by_length=True,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        save_total_limit=3,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        report_to=\"none\",\n",
    "        dataloader_num_workers=0,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "# Create training arguments\n",
    "training_args = create_classification_training_args(\n",
    "    output_dir=f\"./results_{new_model}\",\n",
    "    num_epochs=5\n",
    ")\n",
    "\n",
    "print(\"Classification Training Configuration:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"LR scheduler: {training_args.lr_scheduler_type}\")\n",
    "print(f\"Warmup ratio: {training_args.warmup_ratio}\")\n",
    "print(f\"Weight decay: {training_args.weight_decay}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c555fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom data collator for classification\n",
    "class ClassificationDataCollator:\n",
    "    \"\"\"Custom data collator for classification tasks.\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, max_length=2048):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __call__(self, examples):\n",
    "        batch_texts = [example['text'] for example in examples]\n",
    "        \n",
    "        # Tokenize the batch\n",
    "        tokenized = self.tokenizer(\n",
    "            batch_texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # For causal LM, labels are the same as input_ids\n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "        \n",
    "        return tokenized\n",
    "\n",
    "# Create data collator\n",
    "data_collator = ClassificationDataCollator(tokenizer, max_length=2048)\n",
    "\n",
    "print(\"âœ… Data collator created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dbb55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer with early stopping\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=instruction_datasets['train'],\n",
    "    eval_dataset=instruction_datasets['val'],\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=False,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer initialized with early stopping\")\n",
    "print_memory_usage(\"Before training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b23ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with comprehensive monitoring\n",
    "print(\"Starting pharmaceutical classification training...\")\n",
    "print(\"Expected training time: ~2-3 hours on RTX 4090\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Clear cache and prepare for training\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Training completed in {training_time/3600:.1f} hours!\")\n",
    "print_memory_usage(\"After training\")\n",
    "\n",
    "# Save the trained model\n",
    "trainer.model.save_pretrained(new_model)\n",
    "tokenizer.save_pretrained(new_model)\n",
    "\n",
    "print(f\"\\nâœ… Model saved to ./{new_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34c7c38",
   "metadata": {},
   "source": [
    "## 6. Evaluation Metrics and Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebbf856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation framework\n",
    "class PharmaClassificationEvaluator:\n",
    "    \"\"\"Comprehensive evaluator for pharmaceutical multi-label classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, label_encoder):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_encoder = label_encoder\n",
    "        self.all_labels = list(label_encoder.classes_)\n",
    "    \n",
    "    def predict_labels(self, text: str, max_new_tokens: int = 100) -> List[str]:\n",
    "        \"\"\"Predict labels for a single document.\"\"\"\n",
    "        \n",
    "        # Create instruction format for inference\n",
    "        formatted_text = create_classification_instruction(text, [], is_training=False)\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(formatted_text, return_tensors=\"pt\", truncation=True, max_length=2048).to(device)\n",
    "        \n",
    "        # Generate prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=0.1,  # Low temperature for consistent predictions\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        # Decode prediction\n",
    "        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract labels from response\n",
    "        try:\n",
    "            # Find the labels section\n",
    "            if \"LABELS:\" in full_response:\n",
    "                labels_section = full_response.split(\"LABELS:\")[-1].strip()\n",
    "                # Parse comma-separated labels\n",
    "                predicted_labels = [label.strip() for label in labels_section.split(\",\")]\n",
    "                # Filter valid labels\n",
    "                predicted_labels = [label for label in predicted_labels if label in self.all_labels]\n",
    "            else:\n",
    "                predicted_labels = []\n",
    "        except:\n",
    "            predicted_labels = []\n",
    "        \n",
    "        return predicted_labels\n",
    "    \n",
    "    def evaluate_dataset(self, texts: List[str], true_labels: List[List[str]], \n",
    "                        batch_size: int = 8) -> Dict:\n",
    "        \"\"\"Evaluate the model on a dataset.\"\"\"\n",
    "        \n",
    "        print(f\"Evaluating on {len(texts)} documents...\")\n",
    "        \n",
    "        all_predictions = []\n",
    "        prediction_times = []\n",
    "        \n",
    "        # Process in batches to avoid memory issues\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            batch_predictions = []\n",
    "            \n",
    "            for text in batch_texts:\n",
    "                start_time = time.time()\n",
    "                pred_labels = self.predict_labels(text)\n",
    "                prediction_times.append(time.time() - start_time)\n",
    "                batch_predictions.append(pred_labels)\n",
    "            \n",
    "            all_predictions.extend(batch_predictions)\n",
    "            \n",
    "            if (i // batch_size + 1) % 5 == 0:\n",
    "                print(f\"Processed {i + len(batch_texts)}/{len(texts)} documents\")\n",
    "        \n",
    "        # Convert to binary format for sklearn metrics\n",
    "        y_true = self.label_encoder.transform(true_labels)\n",
    "        y_pred = self.label_encoder.transform(all_predictions)\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        metrics = self._calculate_metrics(y_true, y_pred, true_labels, all_predictions)\n",
    "        metrics['avg_prediction_time'] = np.mean(prediction_times)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _calculate_metrics(self, y_true, y_pred, true_labels_list, pred_labels_list):\n",
    "        \"\"\"Calculate comprehensive evaluation metrics.\"\"\"\n",
    "        \n",
    "        # Overall metrics\n",
    "        micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "        macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "        weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        \n",
    "        micro_precision = precision_score(y_true, y_pred, average='micro')\n",
    "        macro_precision = precision_score(y_true, y_pred, average='macro')\n",
    "        \n",
    "        micro_recall = recall_score(y_true, y_pred, average='micro')\n",
    "        macro_recall = recall_score(y_true, y_pred, average='macro')\n",
    "        \n",
    "        # Multi-label specific metrics\n",
    "        hamming = hamming_loss(y_true, y_pred)\n",
    "        jaccard = jaccard_score(y_true, y_pred, average='macro')\n",
    "        \n",
    "        # Per-label metrics\n",
    "        per_label_f1 = f1_score(y_true, y_pred, average=None)\n",
    "        per_label_precision = precision_score(y_true, y_pred, average=None)\n",
    "        per_label_recall = recall_score(y_true, y_pred, average=None)\n",
    "        \n",
    "        # Exact match accuracy (all labels must match exactly)\n",
    "        exact_matches = sum(1 for true, pred in zip(true_labels_list, pred_labels_list) \n",
    "                           if set(true) == set(pred))\n",
    "        exact_match_ratio = exact_matches / len(true_labels_list)\n",
    "        \n",
    "        return {\n",
    "            'micro_f1': micro_f1,\n",
    "            'macro_f1': macro_f1,\n",
    "            'weighted_f1': weighted_f1,\n",
    "            'micro_precision': micro_precision,\n",
    "            'macro_precision': macro_precision,\n",
    "            'micro_recall': micro_recall,\n",
    "            'macro_recall': macro_recall,\n",
    "            'hamming_loss': hamming,\n",
    "            'jaccard_score': jaccard,\n",
    "            'exact_match_ratio': exact_match_ratio,\n",
    "            'per_label_metrics': {\n",
    "                'labels': list(self.label_encoder.classes_),\n",
    "                'f1_scores': per_label_f1.tolist(),\n",
    "                'precision_scores': per_label_precision.tolist(),\n",
    "                'recall_scores': per_label_recall.tolist()\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = PharmaClassificationEvaluator(model, tokenizer, mlb)\n",
    "print(\"âœ… Evaluator created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad02fd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating fine-tuned model on test set...\")\n",
    "print(\"This may take 15-30 minutes depending on test set size...\")\n",
    "\n",
    "test_metrics = evaluator.evaluate_dataset(\n",
    "    data_splits['test']['texts'],\n",
    "    data_splits['test']['labels'],\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHARMACEUTICAL CLASSIFICATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nðŸ“Š OVERALL PERFORMANCE:\")\n",
    "print(f\"  Micro F1:     {test_metrics['micro_f1']:.4f} ({test_metrics['micro_f1']*100:.1f}%)\")\n",
    "print(f\"  Macro F1:     {test_metrics['macro_f1']:.4f} ({test_metrics['macro_f1']*100:.1f}%)\")\n",
    "print(f\"  Weighted F1:  {test_metrics['weighted_f1']:.4f} ({test_metrics['weighted_f1']*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ PRECISION & RECALL:\")\n",
    "print(f\"  Micro Precision: {test_metrics['micro_precision']:.4f} ({test_metrics['micro_precision']*100:.1f}%)\")\n",
    "print(f\"  Macro Precision: {test_metrics['macro_precision']:.4f} ({test_metrics['macro_precision']*100:.1f}%)\")\n",
    "print(f\"  Micro Recall:    {test_metrics['micro_recall']:.4f} ({test_metrics['micro_recall']*100:.1f}%)\")\n",
    "print(f\"  Macro Recall:    {test_metrics['macro_recall']:.4f} ({test_metrics['macro_recall']*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ MULTI-LABEL METRICS:\")\n",
    "print(f\"  Hamming Loss:      {test_metrics['hamming_loss']:.4f}\")\n",
    "print(f\"  Jaccard Score:     {test_metrics['jaccard_score']:.4f} ({test_metrics['jaccard_score']*100:.1f}%)\")\n",
    "print(f\"  Exact Match Ratio: {test_metrics['exact_match_ratio']:.4f} ({test_metrics['exact_match_ratio']*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nâš¡ PERFORMANCE:\")\n",
    "print(f\"  Avg Prediction Time: {test_metrics['avg_prediction_time']:.3f} seconds\")\n",
    "\n",
    "# Improvement calculation\n",
    "baseline_f1 = 0.80  # Original model performance\n",
    "improvement = (test_metrics['micro_f1'] - baseline_f1) / baseline_f1 * 100\n",
    "\n",
    "print(f\"\\nðŸš€ IMPROVEMENT vs BASELINE:\")\n",
    "print(f\"  Baseline F1:  {baseline_f1:.1%}\")\n",
    "print(f\"  Current F1:   {test_metrics['micro_f1']:.1%}\")\n",
    "print(f\"  Improvement:  {improvement:+.1f}%\")\n",
    "\n",
    "if test_metrics['micro_f1'] >= 0.95:\n",
    "    print(f\"\\nðŸŽ‰ SUCCESS! Target of 95%+ F1 score achieved!\")\n",
    "elif test_metrics['micro_f1'] >= 0.90:\n",
    "    print(f\"\\nâœ… Excellent performance! Close to 95% target.\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ Performance improvement needed for 95% target.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf6ac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed per-label analysis\n",
    "def analyze_per_label_performance(metrics: Dict):\n",
    "    \"\"\"Analyze performance for each label category.\"\"\"\n",
    "    \n",
    "    per_label = metrics['per_label_metrics']\n",
    "    labels = per_label['labels']\n",
    "    f1_scores = per_label['f1_scores']\n",
    "    precision_scores = per_label['precision_scores']\n",
    "    recall_scores = per_label['recall_scores']\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    df = pd.DataFrame({\n",
    "        'Label': labels,\n",
    "        'F1': f1_scores,\n",
    "        'Precision': precision_scores,\n",
    "        'Recall': recall_scores\n",
    "    })\n",
    "    \n",
    "    # Sort by F1 score\n",
    "    df = df.sort_values('F1', ascending=False)\n",
    "    \n",
    "    print(\"\\nPER-LABEL PERFORMANCE ANALYSIS:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Label':<30} {'F1':<8} {'Precision':<10} {'Recall':<8}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Group by category for better analysis\n",
    "    for category, category_labels in PHARMA_LABELS.items():\n",
    "        category_df = df[df['Label'].isin(category_labels)]\n",
    "        if len(category_df) > 0:\n",
    "            print(f\"\\n{category.upper().replace('_', ' ')}:\")\n",
    "            for _, row in category_df.iterrows():\n",
    "                print(f\"{row['Label']:<30} {row['F1']:<8.3f} {row['Precision']:<10.3f} {row['Recall']:<8.3f}\")\n",
    "    \n",
    "    # Identify best and worst performing labels\n",
    "    best_labels = df.head(5)\n",
    "    worst_labels = df[df['F1'] > 0].tail(5)  # Exclude labels with 0 F1\n",
    "    \n",
    "    print(f\"\\nðŸ† TOP 5 PERFORMING LABELS:\")\n",
    "    for _, row in best_labels.iterrows():\n",
    "        print(f\"  {row['Label']}: F1={row['F1']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nâš ï¸ LABELS NEEDING IMPROVEMENT:\")\n",
    "    for _, row in worst_labels.iterrows():\n",
    "        print(f\"  {row['Label']}: F1={row['F1']:.3f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Analyze per-label performance\n",
    "label_performance_df = analyze_per_label_performance(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6658f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance results\n",
    "def visualize_classification_results(metrics: Dict, label_df: pd.DataFrame):\n",
    "    \"\"\"Create comprehensive performance visualizations.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Overall metrics comparison\n",
    "    overall_metrics = {\n",
    "        'Micro F1': metrics['micro_f1'],\n",
    "        'Macro F1': metrics['macro_f1'],\n",
    "        'Weighted F1': metrics['weighted_f1'],\n",
    "        'Jaccard Score': metrics['jaccard_score'],\n",
    "        'Exact Match': metrics['exact_match_ratio']\n",
    "    }\n",
    "    \n",
    "    bars = axes[0, 0].bar(overall_metrics.keys(), overall_metrics.values(), \n",
    "                         color=['skyblue', 'lightgreen', 'orange', 'pink', 'lightyellow'])\n",
    "    axes[0, 0].set_title('Overall Performance Metrics')\n",
    "    axes[0, 0].set_ylabel('Score')\n",
    "    axes[0, 0].set_ylim(0, 1)\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, overall_metrics.values()):\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                       f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # F1 scores by category\n",
    "    top_20_labels = label_df.head(20)\n",
    "    bars = axes[0, 1].barh(range(len(top_20_labels)), top_20_labels['F1'].values, \n",
    "                          color='lightcoral')\n",
    "    axes[0, 1].set_yticks(range(len(top_20_labels)))\n",
    "    axes[0, 1].set_yticklabels(top_20_labels['Label'].values)\n",
    "    axes[0, 1].set_title('Top 20 Labels by F1 Score')\n",
    "    axes[0, 1].set_xlabel('F1 Score')\n",
    "    \n",
    "    # Precision vs Recall scatter\n",
    "    scatter = axes[1, 0].scatter(label_df['Precision'], label_df['Recall'], \n",
    "                                c=label_df['F1'], cmap='viridis', alpha=0.7, s=60)\n",
    "    axes[1, 0].set_xlabel('Precision')\n",
    "    axes[1, 0].set_ylabel('Recall')\n",
    "    axes[1, 0].set_title('Precision vs Recall (colored by F1)')\n",
    "    axes[1, 0].plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=axes[1, 0], label='F1 Score')\n",
    "    \n",
    "    # Comparison with baseline\n",
    "    baseline_scores = [0.80, 0.75, 0.82, 0.60, 0.45]  # Simulated baseline\n",
    "    current_scores = [metrics['micro_f1'], metrics['macro_f1'], metrics['weighted_f1'], \n",
    "                     metrics['jaccard_score'], metrics['exact_match_ratio']]\n",
    "    \n",
    "    x = np.arange(len(overall_metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[1, 1].bar(x - width/2, baseline_scores, width, label='Baseline', color='lightgray')\n",
    "    axes[1, 1].bar(x + width/2, current_scores, width, label='QLoRA Fine-tuned', color='lightblue')\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Metrics')\n",
    "    axes[1, 1].set_ylabel('Score')\n",
    "    axes[1, 1].set_title('Performance Comparison: Baseline vs Fine-tuned')\n",
    "    axes[1, 1].set_xticks(x)\n",
    "    axes[1, 1].set_xticklabels(overall_metrics.keys(), rotation=45)\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Performance by therapeutic area\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    therapeutic_performance = []\n",
    "    for category, category_labels in PHARMA_LABELS.items():\n",
    "        category_df = label_df[label_df['Label'].isin(category_labels)]\n",
    "        if len(category_df) > 0:\n",
    "            avg_f1 = category_df['F1'].mean()\n",
    "            therapeutic_performance.append((category.replace('_', ' ').title(), avg_f1))\n",
    "    \n",
    "    if therapeutic_performance:\n",
    "        categories, f1_scores = zip(*therapeutic_performance)\n",
    "        bars = ax.bar(categories, f1_scores, color='lightseagreen', alpha=0.8)\n",
    "        ax.set_title('Average F1 Score by Label Category')\n",
    "        ax.set_ylabel('Average F1 Score')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, score in zip(bars, f1_scores):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                   f'{score:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create visualizations\n",
    "visualize_classification_results(test_metrics, label_performance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf074513",
   "metadata": {},
   "source": [
    "## 7. Ablation Studies and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be9151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model with sample predictions\n",
    "def test_sample_predictions():\n",
    "    \"\"\"Test the model with sample pharmaceutical documents.\"\"\"\n",
    "    \n",
    "    test_documents = [\n",
    "        {\n",
    "            'text': \"Phase III randomized controlled trial of pembrolizumab versus chemotherapy in patients with advanced non-small cell lung cancer. Primary endpoint was overall survival. 1200 patients enrolled across 150 sites globally. Results demonstrate significant improvement in overall survival with pembrolizumab (HR=0.73, p<0.001). Safety profile consistent with known immune-related adverse events including grade 3 pneumonitis in 2% of patients.\",\n",
    "            'expected_labels': ['oncology', 'phase_3', 'clinical_trial', 'efficacy_study', 'clinical_study_report', 'medium_risk']\n",
    "        },\n",
    "        {\n",
    "            'text': \"Serious adverse event report: 67-year-old female patient experienced severe hepatotoxicity after 14 days of treatment with investigational drug XYZ-123. Event assessed as probably related to study drug. Patient hospitalized and treatment permanently discontinued. Liver function tests showed ALT 15x ULN. Patient recovered with supportive care.\",\n",
    "            'expected_labels': ['adverse_event_report', 'safety_report', 'high_risk', 'pharmacovigilance_report']\n",
    "        },\n",
    "        {\n",
    "            'text': \"INDICATIONS AND USAGE: Adalimumab is a TNF-alpha inhibitor indicated for the treatment of rheumatoid arthritis, Crohn's disease, and psoriasis in adult patients. DOSAGE: 40 mg subcutaneous injection every other week. CONTRAINDICATIONS: Active tuberculosis, serious infections. WARNINGS: Monitor for serious infections and malignancies.\",\n",
    "            'expected_labels': ['drug_label', 'immunology', 'rheumatology', 'post_market', 'medium_risk']\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"SAMPLE PREDICTION TESTING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, doc in enumerate(test_documents, 1):\n",
    "        print(f\"\\nðŸ“„ Test Document {i}:\")\n",
    "        print(f\"Text: {doc['text'][:150]}...\")\n",
    "        \n",
    "        predicted_labels = evaluator.predict_labels(doc['text'])\n",
    "        expected_labels = doc['expected_labels']\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ Expected: {expected_labels}\")\n",
    "        print(f\"ðŸ¤– Predicted: {predicted_labels}\")\n",
    "        \n",
    "        # Calculate overlap\n",
    "        overlap = set(predicted_labels) & set(expected_labels)\n",
    "        precision = len(overlap) / len(predicted_labels) if predicted_labels else 0\n",
    "        recall = len(overlap) / len(expected_labels) if expected_labels else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        print(f\"ðŸ“Š Sample F1: {f1:.3f} (P: {precision:.3f}, R: {recall:.3f})\")\n",
    "        \n",
    "        if f1 > 0.8:\n",
    "            print(\"âœ… Excellent prediction!\")\n",
    "        elif f1 > 0.6:\n",
    "            print(\"âœ… Good prediction\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Needs improvement\")\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Test sample predictions\n",
    "test_sample_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf871656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis and improvement recommendations\n",
    "def analyze_prediction_errors(true_labels: List[List[str]], predicted_labels: List[List[str]]):\n",
    "    \"\"\"Analyze common prediction errors and provide recommendations.\"\"\"\n",
    "    \n",
    "    print(\"ERROR ANALYSIS & IMPROVEMENT RECOMMENDATIONS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Track common errors\n",
    "    false_positives = Counter()\n",
    "    false_negatives = Counter()\n",
    "    label_confusions = Counter()\n",
    "    \n",
    "    for true_set, pred_set in zip(true_labels, predicted_labels):\n",
    "        true_set = set(true_set)\n",
    "        pred_set = set(pred_set)\n",
    "        \n",
    "        # False positives: predicted but not true\n",
    "        fp = pred_set - true_set\n",
    "        for label in fp:\n",
    "            false_positives[label] += 1\n",
    "        \n",
    "        # False negatives: true but not predicted\n",
    "        fn = true_set - pred_set\n",
    "        for label in fn:\n",
    "            false_negatives[label] += 1\n",
    "        \n",
    "        # Label confusions: co-occurring errors\n",
    "        for fp_label in fp:\n",
    "            for fn_label in fn:\n",
    "                label_confusions[(fp_label, fn_label)] += 1\n",
    "    \n",
    "    print(f\"\\nðŸ”´ MOST COMMON FALSE POSITIVES:\")\n",
    "    for label, count in false_positives.most_common(10):\n",
    "        print(f\"  {label}: {count} times\")\n",
    "    \n",
    "    print(f\"\\nðŸ”´ MOST COMMON FALSE NEGATIVES:\")\n",
    "    for label, count in false_negatives.most_common(10):\n",
    "        print(f\"  {label}: {count} times\")\n",
    "    \n",
    "    print(f\"\\nðŸ”„ COMMON LABEL CONFUSIONS:\")\n",
    "    for (fp_label, fn_label), count in label_confusions.most_common(5):\n",
    "        print(f\"  {fp_label} â†” {fn_label}: {count} times\")\n",
    "    \n",
    "    # Improvement recommendations\n",
    "    print(f\"\\nðŸ’¡ IMPROVEMENT RECOMMENDATIONS:\")\n",
    "    \n",
    "    # High false positive rate\n",
    "    top_fp = false_positives.most_common(3)\n",
    "    if top_fp:\n",
    "        print(f\"  1. Reduce false positives for: {', '.join([label for label, _ in top_fp])}\")\n",
    "        print(f\"     â†’ Consider adding negative examples or adjusting decision threshold\")\n",
    "    \n",
    "    # High false negative rate\n",
    "    top_fn = false_negatives.most_common(3)\n",
    "    if top_fn:\n",
    "        print(f\"  2. Improve recall for: {', '.join([label for label, _ in top_fn])}\")\n",
    "        print(f\"     â†’ Add more training examples or improve feature representation\")\n",
    "    \n",
    "    # Class imbalance\n",
    "    total_predictions = sum(false_positives.values()) + sum(false_negatives.values())\n",
    "    if total_predictions > 0:\n",
    "        print(f\"  3. Address class imbalance with balanced sampling or cost-sensitive learning\")\n",
    "        print(f\"  4. Consider ensemble methods or multiple specialized models\")\n",
    "        print(f\"  5. Implement confidence-based filtering for high-stakes predictions\")\n",
    "\n",
    "# Perform error analysis on a subset\n",
    "sample_size = min(100, len(data_splits['test']['texts']))\n",
    "sample_texts = data_splits['test']['texts'][:sample_size]\n",
    "sample_true_labels = data_splits['test']['labels'][:sample_size]\n",
    "\n",
    "print(f\"Performing error analysis on {sample_size} test samples...\")\n",
    "sample_predictions = [evaluator.predict_labels(text) for text in sample_texts]\n",
    "\n",
    "analyze_prediction_errors(sample_true_labels, sample_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ec0b40",
   "metadata": {},
   "source": [
    "## 8. Production Deployment and Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7513c001-3279-444d-afe9-0ed92bc5123e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e091a08d-ebff-4bf8-9470-0f6dd81912fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d047dbf-a254-43e5-ac49-67ec9a484993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
