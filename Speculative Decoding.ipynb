{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speculative Decoding: Accelerating Autoregressive Generation\n",
    "\n",
    "## Overview\n",
    "\n",
    "Speculative decoding is an innovative technique to speed up autoregressive text generation by using a smaller \"draft\" model to predict multiple tokens ahead, which are then verified by a larger \"target\" model. This approach can significantly reduce the number of forward passes through the large model while maintaining the same output quality.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Draft Model**: Small, fast model that generates candidate tokens\n",
    "- **Target Model**: Large, high-quality model that verifies candidates\n",
    "- **Acceptance Rate**: Fraction of draft tokens accepted by target model\n",
    "- **Lookahead Distance**: Number of tokens the draft model predicts ahead\n",
    "\n",
    "### Trade-offs:\n",
    "- **Pros**: Higher throughput, reduced latency for large models\n",
    "- **Cons**: Increased complexity, memory usage, dependency on draft model quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Optional\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Dependencies imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Foundation\n",
    "\n",
    "### Standard Autoregressive Generation\n",
    "In standard autoregressive generation, we sample tokens sequentially:\n",
    "\n",
    "$$x_{t+1} \\sim p_{\\text{target}}(x_{t+1} | x_{1:t})$$\n",
    "\n",
    "This requires one forward pass per token, making generation slow for large models.\n",
    "\n",
    "### Speculative Decoding Algorithm\n",
    "\n",
    "1. **Draft Phase**: Use small model to generate $k$ candidate tokens:\n",
    "   $$\\tilde{x}_{t+1}, \\tilde{x}_{t+2}, ..., \\tilde{x}_{t+k} \\sim p_{\\text{draft}}$$\n",
    "\n",
    "2. **Verification Phase**: Target model computes probabilities for all candidates in parallel:\n",
    "   $$p_{\\text{target}}(\\tilde{x}_{t+i} | x_{1:t}, \\tilde{x}_{t+1:t+i-1})$$\n",
    "\n",
    "3. **Acceptance/Rejection**: For each position $i$:\n",
    "   - Accept $\\tilde{x}_{t+i}$ with probability $\\min(1, \\frac{p_{\\text{target}}(\\tilde{x}_{t+i})}{p_{\\text{draft}}(\\tilde{x}_{t+i})})$\n",
    "   - If rejected, resample from adjusted distribution and stop\n",
    "\n",
    "### Key Insight\n",
    "The algorithm maintains the exact same distribution as standard autoregressive sampling while potentially accepting multiple tokens per target model forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SpeculativeDecodingConfig:\n",
    "    \"\"\"Configuration for speculative decoding.\"\"\"\n",
    "    lookahead_distance: int = 4  # Number of tokens to predict ahead\n",
    "    temperature: float = 1.0     # Sampling temperature\n",
    "    top_k: Optional[int] = None  # Top-k sampling\n",
    "    top_p: Optional[float] = None # Top-p (nucleus) sampling\n",
    "\n",
    "class SimpleLanguageModel(nn.Module):\n",
    "    \"\"\"Simple transformer-like model for demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, d_model: int, n_layers: int, n_heads: int, max_seq_len: int = 512):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "        # Transformer layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        # Create position ids\n",
    "        position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        # Embeddings\n",
    "        token_embeds = self.token_embedding(input_ids)\n",
    "        pos_embeds = self.position_embedding(position_ids)\n",
    "        hidden_states = token_embeds + pos_embeds\n",
    "        \n",
    "        # Create causal mask\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=input_ids.device), diagonal=1).bool()\n",
    "        \n",
    "        # Transformer\n",
    "        hidden_states = self.transformer(hidden_states, mask=causal_mask)\n",
    "        \n",
    "        # Output projection\n",
    "        logits = self.output_projection(hidden_states)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Create toy models for demonstration\n",
    "vocab_size = 1000\n",
    "draft_model = SimpleLanguageModel(vocab_size=vocab_size, d_model=128, n_layers=2, n_heads=4)\n",
    "target_model = SimpleLanguageModel(vocab_size=vocab_size, d_model=256, n_layers=6, n_heads=8)\n",
    "\n",
    "print(f\"Draft model parameters: {sum(p.numel() for p in draft_model.parameters()):,}\")\n",
    "print(f\"Target model parameters: {sum(p.numel() for p in target_model.parameters()):,}\")\n",
    "print(f\"Target model is {sum(p.numel() for p in target_model.parameters()) / sum(p.numel() for p in draft_model.parameters()):.1f}x larger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Utilities\n",
    "\n",
    "We need utilities for sampling from probability distributions with temperature scaling and top-k/top-p filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_temperature(logits: torch.Tensor, temperature: float) -> torch.Tensor:\n",
    "    \"\"\"Apply temperature scaling to logits.\"\"\"\n",
    "    return logits / temperature\n",
    "\n",
    "def top_k_filtering(logits: torch.Tensor, top_k: int) -> torch.Tensor:\n",
    "    \"\"\"Apply top-k filtering to logits.\"\"\"\n",
    "    if top_k <= 0:\n",
    "        return logits\n",
    "    \n",
    "    top_k = min(top_k, logits.size(-1))\n",
    "    values, _ = torch.topk(logits, top_k, dim=-1)\n",
    "    min_values = values[..., -1:]\n",
    "    return torch.where(logits < min_values, torch.full_like(logits, float('-inf')), logits)\n",
    "\n",
    "def top_p_filtering(logits: torch.Tensor, top_p: float) -> torch.Tensor:\n",
    "    \"\"\"Apply top-p (nucleus) filtering to logits.\"\"\"\n",
    "    if top_p >= 1.0:\n",
    "        return logits\n",
    "    \n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n",
    "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "    \n",
    "    # Remove tokens with cumulative probability above the threshold\n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    # Shift the indices to the right to keep also the first token above the threshold\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = 0\n",
    "    \n",
    "    # Scatter sorted indices back to original indexing\n",
    "    indices_to_remove = sorted_indices_to_remove.scatter(dim=-1, index=sorted_indices, src=sorted_indices_to_remove)\n",
    "    logits = logits.masked_fill(indices_to_remove, float('-inf'))\n",
    "    \n",
    "    return logits\n",
    "\n",
    "def sample_from_logits(logits: torch.Tensor, config: SpeculativeDecodingConfig) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Sample tokens from logits with temperature and filtering.\"\"\"\n",
    "    # Apply temperature\n",
    "    logits = apply_temperature(logits, config.temperature)\n",
    "    \n",
    "    # Apply top-k filtering\n",
    "    if config.top_k is not None:\n",
    "        logits = top_k_filtering(logits, config.top_k)\n",
    "    \n",
    "    # Apply top-p filtering\n",
    "    if config.top_p is not None:\n",
    "        logits = top_p_filtering(logits, config.top_p)\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Sample\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    \n",
    "    return next_token, probs\n",
    "\n",
    "print(\"Sampling utilities defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speculative Decoding Implementation\n",
    "\n",
    "Now let's implement the core speculative decoding algorithm. The key insight is that we can verify multiple draft tokens in parallel using the target model, and accept/reject them based on the probability ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeculativeDecoder:\n",
    "    \"\"\"Speculative decoding implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, draft_model: nn.Module, target_model: nn.Module, config: SpeculativeDecodingConfig):\n",
    "        self.draft_model = draft_model\n",
    "        self.target_model = target_model\n",
    "        self.config = config\n",
    "        \n",
    "        # Statistics tracking\n",
    "        self.stats = {\n",
    "            'total_draft_tokens': 0,\n",
    "            'accepted_tokens': 0,\n",
    "            'rejection_positions': [],\n",
    "            'target_model_calls': 0\n",
    "        }\n",
    "    \n",
    "    def reset_stats(self):\n",
    "        \"\"\"Reset statistics tracking.\"\"\"\n",
    "        self.stats = {\n",
    "            'total_draft_tokens': 0,\n",
    "            'accepted_tokens': 0,\n",
    "            'rejection_positions': [],\n",
    "            'target_model_calls': 0\n",
    "        }\n",
    "    \n",
    "    def draft_tokens(self, input_ids: torch.Tensor) -> Tuple[List[int], List[torch.Tensor]]:\n",
    "        \"\"\"Generate draft tokens using the small model.\"\"\"\n",
    "        draft_tokens = []\n",
    "        draft_probs = []\n",
    "        \n",
    "        current_input = input_ids.clone()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(self.config.lookahead_distance):\n",
    "                # Get logits from draft model\n",
    "                logits = self.draft_model(current_input)\n",
    "                next_token_logits = logits[:, -1, :]  # Last position\n",
    "                \n",
    "                # Sample next token\n",
    "                next_token, probs = sample_from_logits(next_token_logits, self.config)\n",
    "                \n",
    "                draft_tokens.append(next_token.item())\n",
    "                draft_probs.append(probs)\n",
    "                \n",
    "                # Append to input for next iteration\n",
    "                current_input = torch.cat([current_input, next_token], dim=1)\n",
    "        \n",
    "        return draft_tokens, draft_probs\n",
    "    \n",
    "    def verify_tokens(self, input_ids: torch.Tensor, draft_tokens: List[int], draft_probs: List[torch.Tensor]) -> Tuple[List[int], int]:\n",
    "        \"\"\"Verify draft tokens using the target model.\"\"\"\n",
    "        # Create extended input with all draft tokens\n",
    "        draft_tensor = torch.tensor(draft_tokens, device=input_ids.device).unsqueeze(0)\n",
    "        extended_input = torch.cat([input_ids, draft_tensor], dim=1)\n",
    "        \n",
    "        # Get target model predictions for all positions\n",
    "        with torch.no_grad():\n",
    "            target_logits = self.target_model(extended_input)\n",
    "            self.stats['target_model_calls'] += 1\n",
    "        \n",
    "        accepted_tokens = []\n",
    "        \n",
    "        # Verify each draft token\n",
    "        for i, (draft_token, draft_prob) in enumerate(zip(draft_tokens, draft_probs)):\n",
    "            # Get target probability for this position\n",
    "            position_idx = input_ids.size(1) + i - 1  # -1 because we predict next token\n",
    "            target_logits_pos = target_logits[:, position_idx, :]\n",
    "            \n",
    "            # Apply same sampling configuration\n",
    "            target_logits_pos = apply_temperature(target_logits_pos, self.config.temperature)\n",
    "            if self.config.top_k is not None:\n",
    "                target_logits_pos = top_k_filtering(target_logits_pos, self.config.top_k)\n",
    "            if self.config.top_p is not None:\n",
    "                target_logits_pos = top_p_filtering(target_logits_pos, self.config.top_p)\n",
    "            \n",
    "            target_probs = F.softmax(target_logits_pos, dim=-1)\n",
    "            \n",
    "            # Calculate acceptance probability\n",
    "            draft_prob_token = draft_prob[0, draft_token]\n",
    "            target_prob_token = target_probs[0, draft_token]\n",
    "            \n",
    "            acceptance_prob = min(1.0, (target_prob_token / (draft_prob_token + 1e-10)).item())\n",
    "            \n",
    "            # Accept or reject\n",
    "            if torch.rand(1).item() < acceptance_prob:\n",
    "                accepted_tokens.append(draft_token)\n",
    "                self.stats['accepted_tokens'] += 1\n",
    "            else:\n",
    "                # Rejection - resample from adjusted distribution\n",
    "                adjusted_probs = torch.clamp(target_probs - draft_prob, min=0.0)\n",
    "                adjusted_probs = adjusted_probs / (adjusted_probs.sum() + 1e-10)\n",
    "                \n",
    "                resampled_token = torch.multinomial(adjusted_probs, num_samples=1)\n",
    "                accepted_tokens.append(resampled_token.item())\n",
    "                self.stats['accepted_tokens'] += 1\n",
    "                self.stats['rejection_positions'].append(i)\n",
    "                break  # Stop at first rejection\n",
    "            \n",
    "            self.stats['total_draft_tokens'] += 1\n",
    "        \n",
    "        return accepted_tokens, len(accepted_tokens)\n",
    "    \n",
    "    def generate_step(self, input_ids: torch.Tensor) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"Perform one speculative decoding step.\"\"\"\n",
    "        # Draft phase\n",
    "        draft_tokens, draft_probs = self.draft_tokens(input_ids)\n",
    "        \n",
    "        # Verification phase\n",
    "        accepted_tokens, num_accepted = self.verify_tokens(input_ids, draft_tokens, draft_probs)\n",
    "        \n",
    "        # Update input\n",
    "        if accepted_tokens:\n",
    "            accepted_tensor = torch.tensor(accepted_tokens, device=input_ids.device).unsqueeze(0)\n",
    "            new_input = torch.cat([input_ids, accepted_tensor], dim=1)\n",
    "        else:\n",
    "            new_input = input_ids\n",
    "        \n",
    "        return new_input, num_accepted\n",
    "    \n",
    "    def generate(self, input_ids: torch.Tensor, max_new_tokens: int) -> torch.Tensor:\n",
    "        \"\"\"Generate text using speculative decoding.\"\"\"\n",
    "        current_input = input_ids.clone()\n",
    "        generated_tokens = 0\n",
    "        \n",
    "        while generated_tokens < max_new_tokens:\n",
    "            current_input, num_accepted = self.generate_step(current_input)\n",
    "            generated_tokens += num_accepted\n",
    "            \n",
    "            if num_accepted == 0:\n",
    "                break  # Fallback to standard generation if needed\n",
    "        \n",
    "        return current_input\n",
    "    \n",
    "    def get_acceptance_rate(self) -> float:\n",
    "        \"\"\"Calculate current acceptance rate.\"\"\"\n",
    "        if self.stats['total_draft_tokens'] == 0:\n",
    "            return 0.0\n",
    "        return self.stats['accepted_tokens'] / self.stats['total_draft_tokens']\n",
    "    \n",
    "    def get_throughput_multiplier(self) -> float:\n",
    "        \"\"\"Calculate throughput multiplier vs standard generation.\"\"\"\n",
    "        if self.stats['target_model_calls'] == 0:\n",
    "            return 1.0\n",
    "        return self.stats['accepted_tokens'] / self.stats['target_model_calls']\n",
    "\n",
    "print(\"Speculative decoder implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Autoregressive Generation (Baseline)\n",
    "\n",
    "Let's implement standard autoregressive generation for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardGenerator:\n",
    "    \"\"\"Standard autoregressive generation for comparison.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, config: SpeculativeDecodingConfig):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.stats = {'model_calls': 0}\n",
    "    \n",
    "    def reset_stats(self):\n",
    "        self.stats = {'model_calls': 0}\n",
    "    \n",
    "    def generate(self, input_ids: torch.Tensor, max_new_tokens: int) -> torch.Tensor:\n",
    "        \"\"\"Generate text using standard autoregressive sampling.\"\"\"\n",
    "        current_input = input_ids.clone()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                # Get next token logits\n",
    "                logits = self.model(current_input)\n",
    "                next_token_logits = logits[:, -1, :]\n",
    "                \n",
    "                self.stats['model_calls'] += 1\n",
    "                \n",
    "                # Sample next token\n",
    "                next_token, _ = sample_from_logits(next_token_logits, self.config)\n",
    "                \n",
    "                # Append to sequence\n",
    "                current_input = torch.cat([current_input, next_token], dim=1)\n",
    "        \n",
    "        return current_input\n",
    "\n",
    "print(\"Standard generator implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Analysis\n",
    "\n",
    "Now let's run experiments to analyze the performance of speculative decoding vs standard generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set models to evaluation mode\n",
    "draft_model.eval()\n",
    "target_model.eval()\n",
    "\n",
    "# Experiment configuration\n",
    "config = SpeculativeDecodingConfig(\n",
    "    lookahead_distance=4,\n",
    "    temperature=0.8,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "# Create generators\n",
    "spec_decoder = SpeculativeDecoder(draft_model, target_model, config)\n",
    "standard_generator = StandardGenerator(target_model, config)\n",
    "\n",
    "# Test input\n",
    "input_text = torch.randint(0, vocab_size, (1, 10))  # Random starting sequence\n",
    "max_new_tokens = 50\n",
    "\n",
    "print(f\"Input shape: {input_text.shape}\")\n",
    "print(f\"Generating {max_new_tokens} new tokens...\")\n",
    "print(f\"Lookahead distance: {config.lookahead_distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run speculative decoding\n",
    "spec_decoder.reset_stats()\n",
    "start_time = time.time()\n",
    "spec_output = spec_decoder.generate(input_text, max_new_tokens)\n",
    "spec_time = time.time() - start_time\n",
    "\n",
    "print(\"Speculative Decoding Results:\")\n",
    "print(f\"Generated sequence length: {spec_output.shape[1]}\")\n",
    "print(f\"New tokens generated: {spec_output.shape[1] - input_text.shape[1]}\")\n",
    "print(f\"Time taken: {spec_time:.3f} seconds\")\n",
    "print(f\"Target model calls: {spec_decoder.stats['target_model_calls']}\")\n",
    "print(f\"Acceptance rate: {spec_decoder.get_acceptance_rate():.3f}\")\n",
    "print(f\"Throughput multiplier: {spec_decoder.get_throughput_multiplier():.2f}x\")\n",
    "print(f\"Rejection positions: {spec_decoder.stats['rejection_positions'][:10]}...\")  # First 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run standard generation\n",
    "standard_generator.reset_stats()\n",
    "start_time = time.time()\n",
    "standard_output = standard_generator.generate(input_text, max_new_tokens)\n",
    "standard_time = time.time() - start_time\n",
    "\n",
    "print(\"Standard Generation Results:\")\n",
    "print(f\"Generated sequence length: {standard_output.shape[1]}\")\n",
    "print(f\"New tokens generated: {standard_output.shape[1] - input_text.shape[1]}\")\n",
    "print(f\"Time taken: {standard_time:.3f} seconds\")\n",
    "print(f\"Target model calls: {standard_generator.stats['model_calls']}\")\n",
    "print(f\"Speedup: {standard_time / spec_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Acceptance Rates\n",
    "\n",
    "Let's analyze how acceptance rates vary with different parameters and model similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_acceptance_rates(lookahead_distances: List[int], temperatures: List[float], num_trials: int = 5):\n",
    "    \"\"\"Analyze acceptance rates across different parameters.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for lookahead in lookahead_distances:\n",
    "        for temperature in temperatures:\n",
    "            config = SpeculativeDecodingConfig(\n",
    "                lookahead_distance=lookahead,\n",
    "                temperature=temperature,\n",
    "                top_k=50\n",
    "            )\n",
    "            \n",
    "            decoder = SpeculativeDecoder(draft_model, target_model, config)\n",
    "            \n",
    "            acceptance_rates = []\n",
    "            throughput_multipliers = []\n",
    "            \n",
    "            for trial in range(num_trials):\n",
    "                decoder.reset_stats()\n",
    "                test_input = torch.randint(0, vocab_size, (1, 8))\n",
    "                decoder.generate(test_input, 30)\n",
    "                \n",
    "                acceptance_rates.append(decoder.get_acceptance_rate())\n",
    "                throughput_multipliers.append(decoder.get_throughput_multiplier())\n",
    "            \n",
    "            results.append({\n",
    "                'lookahead': lookahead,\n",
    "                'temperature': temperature,\n",
    "                'acceptance_rate_mean': np.mean(acceptance_rates),\n",
    "                'acceptance_rate_std': np.std(acceptance_rates),\n",
    "                'throughput_multiplier_mean': np.mean(throughput_multipliers),\n",
    "                'throughput_multiplier_std': np.std(throughput_multipliers)\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run analysis\n",
    "lookahead_distances = [2, 3, 4, 5, 6]\n",
    "temperatures = [0.5, 0.8, 1.0, 1.2]\n",
    "\n",
    "print(\"Running acceptance rate analysis...\")\n",
    "analysis_results = analyze_acceptance_rates(lookahead_distances, temperatures, num_trials=3)\n",
    "print(f\"Completed {len(analysis_results)} experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Convert results to arrays for plotting\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(analysis_results)\n",
    "\n",
    "# 1. Acceptance rate vs lookahead distance\n",
    "for temp in temperatures:\n",
    "    temp_data = df[df['temperature'] == temp]\n",
    "    axes[0, 0].plot(temp_data['lookahead'], temp_data['acceptance_rate_mean'], \n",
    "                   marker='o', label=f'T={temp}')\n",
    "    axes[0, 0].fill_between(temp_data['lookahead'], \n",
    "                           temp_data['acceptance_rate_mean'] - temp_data['acceptance_rate_std'],\n",
    "                           temp_data['acceptance_rate_mean'] + temp_data['acceptance_rate_std'],\n",
    "                           alpha=0.3)\n",
    "\n",
    "axes[0, 0].set_xlabel('Lookahead Distance')\n",
    "axes[0, 0].set_ylabel('Acceptance Rate')\n",
    "axes[0, 0].set_title('Acceptance Rate vs Lookahead Distance')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Throughput multiplier vs lookahead distance\n",
    "for temp in temperatures:\n",
    "    temp_data = df[df['temperature'] == temp]\n",
    "    axes[0, 1].plot(temp_data['lookahead'], temp_data['throughput_multiplier_mean'], \n",
    "                   marker='s', label=f'T={temp}')\n",
    "    axes[0, 1].fill_between(temp_data['lookahead'], \n",
    "                           temp_data['throughput_multiplier_mean'] - temp_data['throughput_multiplier_std'],\n",
    "                           temp_data['throughput_multiplier_mean'] + temp_data['throughput_multiplier_std'],\n",
    "                           alpha=0.3)\n",
    "\n",
    "axes[0, 1].set_xlabel('Lookahead Distance')\n",
    "axes[0, 1].set_ylabel('Throughput Multiplier')\n",
    "axes[0, 1].set_title('Throughput Multiplier vs Lookahead Distance')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Acceptance rate vs temperature\n",
    "for lookahead in lookahead_distances:\n",
    "    lookahead_data = df[df['lookahead'] == lookahead]\n",
    "    axes[1, 0].plot(lookahead_data['temperature'], lookahead_data['acceptance_rate_mean'], \n",
    "                   marker='o', label=f'L={lookahead}')\n",
    "\n",
    "axes[1, 0].set_xlabel('Temperature')\n",
    "axes[1, 0].set_ylabel('Acceptance Rate')\n",
    "axes[1, 0].set_title('Acceptance Rate vs Temperature')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Heatmap of acceptance rates\n",
    "pivot_data = df.pivot(index='temperature', columns='lookahead', values='acceptance_rate_mean')\n",
    "sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='viridis', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Acceptance Rate Heatmap')\n",
    "axes[1, 1].set_xlabel('Lookahead Distance')\n",
    "axes[1, 1].set_ylabel('Temperature')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(f\"Best acceptance rate: {df['acceptance_rate_mean'].max():.3f} (T={df.loc[df['acceptance_rate_mean'].idxmax(), 'temperature']}, L={df.loc[df['acceptance_rate_mean'].idxmax(), 'lookahead']})\")\n",
    "print(f\"Best throughput multiplier: {df['throughput_multiplier_mean'].max():.2f}x (T={df.loc[df['throughput_multiplier_mean'].idxmax(), 'temperature']}, L={df.loc[df['throughput_multiplier_mean'].idxmax(), 'lookahead']})\")\n",
    "print(f\"Mean acceptance rate: {df['acceptance_rate_mean'].mean():.3f} ± {df['acceptance_rate_mean'].std():.3f}\")\n",
    "print(f\"Mean throughput multiplier: {df['throughput_multiplier_mean'].mean():.2f}x ± {df['throughput_multiplier_mean'].std():.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Rejection Patterns\n",
    "\n",
    "Let's analyze where rejections typically occur in the lookahead sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_rejection_patterns(lookahead_distance: int = 4, num_samples: int = 100):\n",
    "    \"\"\"Analyze rejection patterns in speculative decoding.\"\"\"\n",
    "    config = SpeculativeDecodingConfig(\n",
    "        lookahead_distance=lookahead_distance,\n",
    "        temperature=1.0,\n",
    "        top_k=50\n",
    "    )\n",
    "    \n",
    "    decoder = SpeculativeDecoder(draft_model, target_model, config)\n",
    "    decoder.reset_stats()\n",
    "    \n",
    "    # Run multiple generation steps\n",
    "    for _ in range(num_samples):\n",
    "        test_input = torch.randint(0, vocab_size, (1, 10))\n",
    "        decoder.generate_step(test_input)\n",
    "    \n",
    "    return decoder.stats['rejection_positions']\n",
    "\n",
    "# Analyze rejection patterns\n",
    "rejection_positions = analyze_rejection_patterns(lookahead_distance=5, num_samples=200)\n",
    "\n",
    "# Plot rejection position distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogram of rejection positions\n",
    "if rejection_positions:\n",
    "    ax1.hist(rejection_positions, bins=range(6), alpha=0.7, edgecolor='black')\n",
    "    ax1.set_xlabel('Rejection Position in Lookahead Sequence')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_title('Distribution of Rejection Positions')\n",
    "    ax1.set_xticks(range(5))\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean_rejection_pos = np.mean(rejection_positions)\n",
    "    median_rejection_pos = np.median(rejection_positions)\n",
    "    \n",
    "    ax1.axvline(mean_rejection_pos, color='red', linestyle='--', label=f'Mean: {mean_rejection_pos:.2f}')\n",
    "    ax1.axvline(median_rejection_pos, color='orange', linestyle='--', label=f'Median: {median_rejection_pos:.2f}')\n",
    "    ax1.legend()\n",
    "    \n",
    "    print(f\"Rejection statistics (out of {len(rejection_positions)} rejections):\")\n",
    "    print(f\"Mean rejection position: {mean_rejection_pos:.2f}\")\n",
    "    print(f\"Median rejection position: {median_rejection_pos:.2f}\")\n",
    "    print(f\"Most common rejection position: {max(set(rejection_positions), key=rejection_positions.count)}\")\n",
    "else:\n",
    "    ax1.text(0.5, 0.5, 'No rejections occurred', transform=ax1.transAxes, \n",
    "             ha='center', va='center', fontsize=14)\n",
    "    ax1.set_title('Distribution of Rejection Positions (No Rejections)')\n",
    "\n",
    "# Cumulative acceptance probability by position\n",
    "total_positions = len(rejection_positions) + 200 * 5  # Approximate total positions tried\n",
    "position_counts = [rejection_positions.count(i) for i in range(5)]\n",
    "cumulative_acceptance = []\n",
    "cumulative_positions = 0\n",
    "\n",
    "for i in range(5):\n",
    "    cumulative_positions += 200  # Each generation step tries position i\n",
    "    cumulative_rejections = sum(position_counts[:i+1])\n",
    "    acceptance_rate = 1 - (cumulative_rejections / cumulative_positions)\n",
    "    cumulative_acceptance.append(acceptance_rate)\n",
    "\n",
    "ax2.plot(range(5), cumulative_acceptance, marker='o', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Position in Lookahead Sequence')\n",
    "ax2.set_ylabel('Cumulative Acceptance Rate')\n",
    "ax2.set_title('Acceptance Rate by Position')\n",
    "ax2.set_xticks(range(5))\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Similarity Analysis\n",
    "\n",
    "The effectiveness of speculative decoding depends heavily on how similar the draft and target models are. Let's create models with different levels of similarity and analyze the impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_similar_model(base_model: nn.Module, similarity_factor: float) -> nn.Module:\n",
    "    \"\"\"Create a model similar to the base model by copying and adding noise to weights.\"\"\"\n",
    "    similar_model = SimpleLanguageModel(\n",
    "        vocab_size=base_model.vocab_size,\n",
    "        d_model=base_model.d_model // 2,  # Smaller model\n",
    "        n_layers=max(1, base_model.transformer.num_layers // 2),\n",
    "        n_heads=max(1, base_model.transformer.layers[0].self_attn.num_heads // 2)\n",
    "    )\n",
    "    \n",
    "    # Initialize with noise based on similarity factor\n",
    "    with torch.no_grad():\n",
    "        for param in similar_model.parameters():\n",
    "            # Start with random initialization\n",
    "            noise = torch.randn_like(param) * (1 - similarity_factor)\n",
    "            # Add some structured similarity (simplified)\n",
    "            base_influence = torch.randn_like(param) * similarity_factor * 0.1\n",
    "            param.data = noise + base_influence\n",
    "    \n",
    "    return similar_model\n",
    "\n",
    "def test_model_similarity_impact(similarity_factors: List[float]):\n",
    "    \"\"\"Test how model similarity affects speculative decoding performance.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    config = SpeculativeDecodingConfig(\n",
    "        lookahead_distance=4,\n",
    "        temperature=1.0,\n",
    "        top_k=50\n",
    "    )\n",
    "    \n",
    "    for similarity in similarity_factors:\n",
    "        # Create draft model with specified similarity\n",
    "        draft = create_similar_model(target_model, similarity)\n",
    "        draft.eval()\n",
    "        \n",
    "        decoder = SpeculativeDecoder(draft, target_model, config)\n",
    "        \n",
    "        # Test multiple times\n",
    "        acceptance_rates = []\n",
    "        throughput_multipliers = []\n",
    "        \n",
    "        for _ in range(5):\n",
    "            decoder.reset_stats()\n",
    "            test_input = torch.randint(0, vocab_size, (1, 8))\n",
    "            decoder.generate(test_input, 20)\n",
    "            \n",
    "            acceptance_rates.append(decoder.get_acceptance_rate())\n",
    "            throughput_multipliers.append(decoder.get_throughput_multiplier())\n",
    "        \n",
    "        results.append({\n",
    "            'similarity': similarity,\n",
    "            'acceptance_rate': np.mean(acceptance_rates),\n",
    "            'acceptance_rate_std': np.std(acceptance_rates),\n",
    "            'throughput_multiplier': np.mean(throughput_multipliers),\n",
    "            'throughput_multiplier_std': np.std(throughput_multipliers)\n",
    "        })\n",
    "        \n",
    "        print(f\"Similarity {similarity:.2f}: Acceptance rate = {np.mean(acceptance_rates):.3f} ± {np.std(acceptance_rates):.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test different similarity levels\n",
    "similarity_factors = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "print(\"Testing impact of model similarity on speculative decoding...\")\n",
    "similarity_results = test_model_similarity_impact(similarity_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot similarity analysis results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "similarities = [r['similarity'] for r in similarity_results]\n",
    "acceptance_rates = [r['acceptance_rate'] for r in similarity_results]\n",
    "acceptance_stds = [r['acceptance_rate_std'] for r in similarity_results]\n",
    "throughput_multipliers = [r['throughput_multiplier'] for r in similarity_results]\n",
    "throughput_stds = [r['throughput_multiplier_std'] for r in similarity_results]\n",
    "\n",
    "# Acceptance rate vs similarity\n",
    "ax1.errorbar(similarities, acceptance_rates, yerr=acceptance_stds, \n",
    "             marker='o', linewidth=2, markersize=8, capsize=5)\n",
    "ax1.set_xlabel('Model Similarity Factor')\n",
    "ax1.set_ylabel('Acceptance Rate')\n",
    "ax1.set_title('Acceptance Rate vs Model Similarity')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim([-0.05, 1.05])\n",
    "\n",
    "# Throughput multiplier vs similarity\n",
    "ax2.errorbar(similarities, throughput_multipliers, yerr=throughput_stds, \n",
    "             marker='s', linewidth=2, markersize=8, capsize=5, color='orange')\n",
    "ax2.set_xlabel('Model Similarity Factor')\n",
    "ax2.set_ylabel('Throughput Multiplier')\n",
    "ax2.set_title('Throughput Multiplier vs Model Similarity')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim([-0.05, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(f\"- Highest acceptance rate: {max(acceptance_rates):.3f} at similarity {similarities[acceptance_rates.index(max(acceptance_rates))]:.2f}\")\n",
    "print(f\"- Highest throughput: {max(throughput_multipliers):.2f}x at similarity {similarities[throughput_multipliers.index(max(throughput_multipliers))]:.2f}\")\n",
    "print(f\"- Acceptance rate range: {min(acceptance_rates):.3f} - {max(acceptance_rates):.3f}\")\n",
    "print(f\"- Throughput range: {min(throughput_multipliers):.2f}x - {max(throughput_multipliers):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trade-offs and Practical Considerations\n",
    "\n",
    "Let's analyze the key trade-offs in speculative decoding implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tradeoffs():\n",
    "    \"\"\"Analyze key trade-offs in speculative decoding.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"SPECULATIVE DECODING TRADE-OFFS ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\n1. COMPUTATIONAL OVERHEAD:\")\n",
    "    print(\"   Pros:\")\n",
    "    print(\"   + Reduced target model forward passes\")\n",
    "    print(\"   + Parallel verification of multiple tokens\")\n",
    "    print(\"   + Potential for significant speedup (1.5-3x typical)\")\n",
    "    print(\"   \\n   Cons:\")\n",
    "    print(\"   - Draft model overhead (additional compute)\")\n",
    "    print(\"   - Memory overhead for storing draft sequences\")\n",
    "    print(\"   - Implementation complexity\")\n",
    "    \n",
    "    print(\"\\n2. QUALITY GUARANTEES:\")\n",
    "    print(\"   Pros:\")\n",
    "    print(\"   + Mathematically equivalent to standard sampling\")\n",
    "    print(\"   + No quality degradation when properly implemented\")\n",
    "    print(\"   + Same distribution as target model\")\n",
    "    print(\"   \\n   Cons:\")\n",
    "    print(\"   - Requires careful implementation of rejection sampling\")\n",
    "    print(\"   - Numerical precision considerations\")\n",
    "    \n",
    "    print(\"\\n3. PERFORMANCE FACTORS:\")\n",
    "    print(\"   Critical Dependencies:\")\n",
    "    print(\"   - Draft model quality (higher similarity = better performance)\")\n",
    "    print(\"   - Lookahead distance (optimal range: 3-6 tokens)\")\n",
    "    print(\"   - Temperature settings (lower temp often better)\")\n",
    "    print(\"   - Hardware characteristics (memory bandwidth, compute ratio)\")\n",
    "    \n",
    "    print(\"\\n4. PRACTICAL DEPLOYMENT:\")\n",
    "    print(\"   Considerations:\")\n",
    "    print(\"   - Draft model training/distillation requirements\")\n",
    "    print(\"   - Memory usage (storing multiple model states)\")\n",
    "    print(\"   - Batching strategies (more complex with variable acceptance)\")\n",
    "    print(\"   - Hardware optimization opportunities\")\n",
    "    \n",
    "    print(\"\\n5. WHEN TO USE:\")\n",
    "    print(\"   Recommended:\")\n",
    "    print(\"   - Latency-critical applications\")\n",
    "    print(\"   - Large target models with available smaller variants\")\n",
    "    print(\"   - Interactive applications (chatbots, code completion)\")\n",
    "    print(\"   \\n   Not Recommended:\")\n",
    "    print(\"   - Very small target models (overhead may dominate)\")\n",
    "    print(\"   - When draft model quality is poor\")\n",
    "    print(\"   - Memory-constrained environments\")\n",
    "\n",
    "analyze_tradeoffs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "Based on our implementation and analysis, here are the key findings about speculative decoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_report():\n",
    "    \"\"\"Generate a comprehensive summary of findings.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"SPECULATIVE DECODING: SUMMARY REPORT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\n🎯 KEY ALGORITHM INSIGHTS:\")\n",
    "    print(\"\\n1. Probabilistic Acceptance: The algorithm maintains exact equivalence\")\n",
    "    print(\"   to standard autoregressive sampling through careful rejection sampling.\")\n",
    "    \n",
    "    print(\"\\n2. Parallel Verification: Multiple draft tokens can be verified in a\")\n",
    "    print(\"   single target model forward pass, enabling significant speedups.\")\n",
    "    \n",
    "    print(\"\\n3. Graceful Degradation: Even with poor draft models, the algorithm\")\n",
    "    print(\"   maintains correctness and often still provides some speedup.\")\n",
    "    \n",
    "    print(\"\\n📊 PERFORMANCE CHARACTERISTICS:\")\n",
    "    print(\"\\n• Typical speedups: 1.5-3x for well-matched draft/target pairs\")\n",
    "    print(\"• Acceptance rates: 30-80% depending on model similarity\")\n",
    "    print(\"• Memory overhead: ~20-50% increase for storing draft states\")\n",
    "    print(\"• Optimal lookahead: 3-6 tokens for most scenarios\")\n",
    "    \n",
    "    print(\"\\n🔧 IMPLEMENTATION BEST PRACTICES:\")\n",
    "    print(\"\\n1. Draft Model Selection:\")\n",
    "    print(\"   - Use models trained on similar data as target\")\n",
    "    print(\"   - 4-10x smaller parameter count typically optimal\")\n",
    "    print(\"   - Consider knowledge distillation for better alignment\")\n",
    "    \n",
    "    print(\"\\n2. Hyperparameter Tuning:\")\n",
    "    print(\"   - Lower temperatures generally improve acceptance rates\")\n",
    "    print(\"   - Lookahead distance should be tuned based on hardware\")\n",
    "    print(\"   - Top-k/top-p filtering affects both models equally\")\n",
    "    \n",
    "    print(\"\\n3. System Optimization:\")\n",
    "    print(\"   - Batch processing requires careful sequence alignment\")\n",
    "    print(\"   - Memory-efficient implementations crucial for deployment\")\n",
    "    print(\"   - Hardware-specific optimizations can provide additional gains\")\n",
    "    \n",
    "    print(\"\\n🚀 FUTURE DIRECTIONS:\")\n",
    "    print(\"\\n• Adaptive lookahead based on real-time acceptance rates\")\n",
    "    print(\"• Multi-level speculative decoding with cascaded draft models\")\n",
    "    print(\"• Integration with other acceleration techniques (quantization, etc.)\")\n",
    "    print(\"• Specialized hardware designs for speculative execution\")\n",
    "    \n",
    "    print(\"\\n✅ CONCLUSION:\")\n",
    "    print(\"\\nSpeculative decoding represents a clever probabilistic approach to\")\n",
    "    print(\"accelerating autoregressive generation. When properly implemented with\")\n",
    "    print(\"well-matched draft models, it can provide substantial speedups while\")\n",
    "    print(\"maintaining perfect output quality. The technique is particularly\")\n",
    "    print(\"valuable for interactive applications where latency is critical.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "generate_summary_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Implementing Improvements\n",
    "\n",
    "Try implementing these extensions to deepen your understanding:\n",
    "\n",
    "1. **Adaptive Lookahead**: Modify the decoder to adjust lookahead distance based on recent acceptance rates\n",
    "\n",
    "2. **Batch Processing**: Extend the implementation to handle multiple sequences simultaneously\n",
    "\n",
    "3. **Multiple Draft Models**: Implement a version that uses multiple draft models of different sizes\n",
    "\n",
    "4. **Performance Profiling**: Add detailed timing and memory usage tracking\n",
    "\n",
    "5. **Real Model Integration**: Adapt the code to work with actual pre-trained models (e.g., GPT-2 variants)\n",
    "\n",
    "6. **Tree-based Speculation**: Implement tree-based speculative decoding for handling multiple possible futures\n",
    "\n",
    "Each of these extensions would provide deeper insights into the practical challenges and opportunities in deploying speculative decoding in production systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
